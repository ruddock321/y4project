{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "file_path = 'C:\\\\Users\\\\Dell\\\\Downloads\\\\Garstec_AS09_chiara.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "ages = []\n",
    "massini = []\n",
    "fehini = []\n",
    "alphamlt = []\n",
    "yini = []\n",
    "eta = []\n",
    "alphafe = []\n",
    "\n",
    "# Outputs\n",
    "teff = []\n",
    "luminosity = []\n",
    "dnufit = []\n",
    "FeH = []\n",
    "G_GAIA = []\n",
    "massfin = []\n",
    "numax = []\n",
    "MeH = []\n",
    "\n",
    "\n",
    "# Open the hdf5 file (read-only mode)\n",
    "with h5py.File(file_path, 'r') as hdf:\n",
    "\n",
    "    grid = hdf['grid']\n",
    "    tracks = grid['tracks']\n",
    "\n",
    "    # Get a list of track names and shuffle for random sampling\n",
    "    track_names = list(tracks.keys())\n",
    "    random.seed(1)\n",
    "    random.shuffle(track_names)\n",
    "\n",
    "    # Choose a subset of tracks to process\n",
    "    num_tracks = 1000  # Set the number of tracks to process\n",
    "    selected_tracks = track_names[:num_tracks]\n",
    "\n",
    "    for track_name in selected_tracks:  # Iterate over the selected track names\n",
    "        track = tracks[track_name]\n",
    "        \n",
    "        # Inputs\n",
    "        epsilon = 1e-10  # Small constant to handle zero values\n",
    "        age_data = np.sign(track['age'][:]) * np.log(np.abs(track['age'][:]) + epsilon)\n",
    "        mass_data = np.sign(track['massini'][:]) * np.log(np.abs(track['massini'][:]) + epsilon)\n",
    "        FeHini_data = np.sign(track['FeHini'][:]) * np.log(np.abs(track['FeHini'][:]) + epsilon)\n",
    "        alphaMLT_data = np.sign(track['alphaMLT'][:]) * np.log(np.abs(track['alphaMLT'][:]) + epsilon)\n",
    "        yini_data = np.sign(track['yini'][:]) * np.log(np.abs(track['yini'][:]) + epsilon)\n",
    "        eta_data = np.sign(track['eta'][:]) * np.log(np.abs(track['eta'][:]) + epsilon)\n",
    "        alphaFe_data = np.sign(track['alphaFe'][:]) * np.log(np.abs(track['alphaFe'][:]) + epsilon)\n",
    "\n",
    "        ages.append(age_data)\n",
    "        massini.append(mass_data)\n",
    "        fehini.append(FeHini_data)\n",
    "        alphamlt.append(alphaMLT_data)\n",
    "        yini.append(yini_data)\n",
    "        eta.append(eta_data)\n",
    "        alphafe.append(alphaFe_data)\n",
    "\n",
    "        # Outputs\n",
    "        teff.append(track['Teff'][:])\n",
    "        luminosity.append(track['LPhot'][:])\n",
    "        dnufit.append(track['dnufit'][:])\n",
    "        FeH.append(track['FeH'][:])\n",
    "        G_GAIA.append(track['G_GAIA'][:])\n",
    "        massfin.append(track['massfin'][:])\n",
    "        numax.append(track['numax'][:])\n",
    "        MeH.append(track['MeH'][:])\n",
    "\n",
    "# Convert lists to numpy arrays and concatenate them (make one big list)\n",
    "\n",
    "# Inputs\n",
    "ages = np.concatenate(ages).reshape(-1, 1)\n",
    "massini = np.concatenate(massini).reshape(-1, 1)\n",
    "fehini = np.concatenate(fehini).reshape(-1, 1)\n",
    "alphamlt = np.concatenate(alphamlt).reshape(-1, 1)\n",
    "yini = np.concatenate(yini).reshape(-1, 1)\n",
    "eta = np.concatenate(eta).reshape(-1, 1)\n",
    "alphafe = np.concatenate(alphafe).reshape(-1, 1)\n",
    "\n",
    "# Outputs\n",
    "teff = np.concatenate(teff)\n",
    "luminosity = np.concatenate(luminosity)\n",
    "dnufit = np.concatenate(dnufit).reshape(-1, 1)\n",
    "FeH = np.concatenate(FeH).reshape(-1, 1)\n",
    "G_GAIA = np.concatenate(G_GAIA).reshape(-1, 1)\n",
    "massfin = np.concatenate(massfin).reshape(-1, 1)\n",
    "numax = np.concatenate(numax).reshape(-1, 1)\n",
    "MeH = np.concatenate(MeH).reshape(-1, 1)\n",
    "\n",
    "# Take the log of Teff and LPhot\n",
    "log_teff = np.log10(teff).reshape(-1, 1)\n",
    "log_luminosity = np.log10(luminosity).reshape(-1, 1)\n",
    "\n",
    "# Combine all inputs into a single array\n",
    "inputs = np.column_stack((ages, massini, fehini, alphamlt, yini, eta, alphafe))\n",
    "\n",
    "# Combine Teff and LPhot as outputs\n",
    "outputs = np.column_stack((log_teff, log_luminosity, dnufit, FeH, massfin, numax, MeH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler()\n",
    "Y_scaler = StandardScaler()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(inputs, outputs, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train = torch.FloatTensor(X_scaler.fit_transform(X_train))\n",
    "X_test = torch.FloatTensor(X_scaler.transform(X_test))\n",
    "Y_train = torch.FloatTensor(Y_scaler.fit_transform(Y_train))\n",
    "Y_test = torch.FloatTensor(Y_scaler.transform(Y_test))\n",
    "\n",
    "dataset = TensorDataset(X_train, Y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class StellarModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StellarModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(7, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 7)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the model, define loss and optimizer\n",
    "model = StellarModel()\n",
    "criterion = nn.MSELoss()\n",
    "lr = 0.005\n",
    "step_size = 500\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.9, last_epoch=-1)\n",
    "\n",
    "num_epochs = 600\n",
    "train_loss = []  \n",
    "test_loss = []   \n",
    "learning_rate = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/600], Train Loss: 0.4593, Test Loss: 0.4570\n",
      "Epoch [200/600], Train Loss: 0.4716, Test Loss: 0.4720\n",
      "Epoch [300/600], Train Loss: 0.4587, Test Loss: 0.4567\n",
      "Epoch [400/600], Train Loss: 0.4727, Test Loss: 0.4702\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization for training data\u001b[39;00m\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear gradients\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m epoch_train_loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()       \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Forward pass for test data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    523\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[0;32m    290\u001b[0m     tensors,\n\u001b[0;32m    291\u001b[0m     grad_tensors_,\n\u001b[0;32m    292\u001b[0m     retain_graph,\n\u001b[0;32m    293\u001b[0m     create_graph,\n\u001b[0;32m    294\u001b[0m     inputs,\n\u001b[0;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    297\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    # Forward pass for training data\n",
    "    outputs_train = model(X_train)\n",
    "    epoch_train_loss = criterion(outputs_train, Y_train)\n",
    "\n",
    "    # Backward pass and optimization for training data\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    epoch_train_loss.backward()  # Backpropagation\n",
    "    optimizer.step()       # Update weights\n",
    "    \n",
    "\n",
    "    # Forward pass for test data\n",
    "    model.eval()  # Set the model to evaluation mode (disables dropout, etc.)\n",
    "    with torch.no_grad():\n",
    "        outputs_test = model(X_test)\n",
    "        epoch_test_loss = criterion(outputs_test, Y_test)  # Calculate test loss\n",
    "    \n",
    "    scheduler.step()\n",
    "    lr_after  = optimizer.param_groups[0][\"lr\"]\n",
    "    # Store the losses for plotting\n",
    "    train_loss.append(epoch_train_loss.item())\n",
    "    test_loss.append(epoch_test_loss.item())\n",
    "    learning_rate.append(lr_after)\n",
    "\n",
    "    # Print progress every 1000 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_train_loss.item():.4f}, Test Loss: {epoch_test_loss.item():.4f}')\n",
    "\n",
    "# Plot training and test loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.log10(train_loss), label='Training Loss (log scale)')\n",
    "plt.plot(np.log10(test_loss), label='Test Loss (log scale)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('Log Loss vs. Epoch (Training vs. Test)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(learning_rate)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.title('Learning rate vs. Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), 'garstec_model_stateV2.pth')\n",
    "\n",
    "# Save the optimizer's state dictionary\n",
    "torch.save(optimizer.state_dict(), 'garstec_optimizer_stateV2.pth')\n",
    "\n",
    "# Save the entire model\n",
    "torch.save(model, 'garstec_entire_modelV2.pth')\n",
    "\n",
    "print(\"Model, optimizer, and entire model saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
