{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import jax\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS, Predictive\n",
    "import corner\n",
    "\n",
    "garstec_data = 'C:\\\\Users\\\\Dell\\\\Downloads\\\\Garstec_AS09_chiara.hdf5'\n",
    "\n",
    "sun_numax = 3090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GarstecNet(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=7, out_features=512, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (9): LeakyReLU(negative_slope=0.01)\n",
       "    (10): Linear(in_features=32, out_features=5, bias=True)\n",
       "  )\n",
       "  (criterion): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GarstecNet(LightningModule):\n",
    "    def __init__(self, input_dim, output_dim, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = lr\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),  # First layer maps input_dim to 256 neurons\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 256),  # 2\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 128),  # 3\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 64),  # 4\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32),  # 5\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, output_dim)  # Output layer\n",
    "        )\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=25, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1\n",
    "            },\n",
    "        }\n",
    "    \n",
    "ckpt_path = \"best_model_v9-1-SS---epoch=8563-val_loss=0.00008340.ckpt\" \n",
    "input_dim = 7  # Number of input features\n",
    "output_dim = 5  # Number of output features\n",
    "model = GarstecNet.load_from_checkpoint(ckpt_path, input_dim=input_dim, output_dim=output_dim)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "scaler_X = joblib.load('scalerXV9.pkl')\n",
    "scaler_y = joblib.load('scalerYV9.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "# List of 10 specific track names\n",
    "track_names = ['track01624', 'track01975', 'track08968', 'track02584', 'track04056',\n",
    "               'track05847', 'track05944', 'track06984', 'track07032', 'track07783']\n",
    "\n",
    "print(len(track_names))\n",
    "\n",
    "# Dictionary to store extracted data\n",
    "star_data = {}\n",
    "\n",
    "# Open HDF5 file and extract data for each track\n",
    "with h5py.File(garstec_data, 'r') as hdf:\n",
    "    for track_name in track_names:\n",
    "        specific_track = hdf['grid']['tracks'][track_name]\n",
    "\n",
    "        # Extract parameters and reshape where needed\n",
    "        star_data[track_name] = {\n",
    "            'age': specific_track['age'][:].reshape(-1, 1),\n",
    "            'massini': specific_track['massini'][:].reshape(-1, 1),\n",
    "            'fehini': specific_track['FeHini'][:].reshape(-1, 1),\n",
    "            'alphamlt': specific_track['alphaMLT'][:].reshape(-1, 1),\n",
    "            'yini': specific_track['yini'][:].reshape(-1, 1),\n",
    "            'eta': specific_track['eta'][:].reshape(-1, 1),\n",
    "            'alphafe': specific_track['alphaFe'][:].reshape(-1, 1),\n",
    "\n",
    "            # Observational values (no reshape needed)\n",
    "            'teff': specific_track['Teff'][:],\n",
    "            'luminosity': specific_track['LPhot'][:],\n",
    "            'dnufit': specific_track['dnufit'][:],\n",
    "            'FeH': specific_track['FeH'][:],\n",
    "            'numax': specific_track['numax'][:]\n",
    "        }\n",
    "\n",
    "        # Select middle age index\n",
    "        index = len(star_data[track_name]['age']) // 2\n",
    "\n",
    "        # Extract single values at the middle index\n",
    "        star_data[track_name]['selected'] = {\n",
    "            'age': star_data[track_name]['age'][index],\n",
    "            'massini': star_data[track_name]['massini'][index],\n",
    "            'fehini': star_data[track_name]['fehini'][index],\n",
    "            'alphamlt': star_data[track_name]['alphamlt'][index],\n",
    "            'yini': star_data[track_name]['yini'][index],\n",
    "            'eta': star_data[track_name]['eta'][index],\n",
    "            'alphafe': star_data[track_name]['alphafe'][index],\n",
    "\n",
    "            'teff': star_data[track_name]['teff'][index],\n",
    "            'luminosity': star_data[track_name]['luminosity'][index],\n",
    "            'dnufit': star_data[track_name]['dnufit'][index],\n",
    "            'FeH': star_data[track_name]['FeH'][index],\n",
    "            'numax': star_data[track_name]['numax'][index]\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results\n",
    "predictions_dict = {}\n",
    "epsilon = 1e-10\n",
    "\n",
    "# Iterate through each track\n",
    "for track_name in track_names:\n",
    "    # Extract the stored middle index data for each star\n",
    "    age = star_data[track_name]['selected']['age']\n",
    "    massini = star_data[track_name]['selected']['massini']\n",
    "    fehini = star_data[track_name]['selected']['fehini']\n",
    "    alphamlt = star_data[track_name]['selected']['alphamlt']\n",
    "    yini = star_data[track_name]['selected']['yini']\n",
    "    eta = star_data[track_name]['selected']['eta']\n",
    "    alphafe = star_data[track_name]['selected']['alphafe']\n",
    "\n",
    "    # Apply log10 transformation where necessary\n",
    "    log10_inputs = [age, massini, alphamlt, eta, yini]\n",
    "    log10_transformed_inputs = [np.log10(np.maximum(data, epsilon)) for data in log10_inputs]\n",
    "\n",
    "    # Combine log-transformed inputs with raw `fehini` and `alphafe`\n",
    "    features_real = np.hstack(log10_transformed_inputs + [fehini, alphafe])\n",
    "\n",
    "    # Convert to NumPy array for scaling\n",
    "    features_numpy_real = np.array(features_real).reshape(1, -1)  # Ensure correct shape\n",
    "\n",
    "    # Apply scaling\n",
    "    features_scaled_real = scaler_X.transform(features_numpy_real)\n",
    "\n",
    "    # Convert scaled inputs to PyTorch tensor\n",
    "    features_tensor_real = torch.FloatTensor(features_scaled_real)\n",
    "\n",
    "    # Make predictions using the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(features_tensor_real).numpy()  # Predict\n",
    "        predictions = scaler_y.inverse_transform(predictions)  # Inverse transform\n",
    "\n",
    "    # Extract predicted values\n",
    "    teff = 10**predictions[:, 0]  # Inverse log10 transformation\n",
    "    luminosity = 10**predictions[:, 1]\n",
    "    dnu = 10**predictions[:, 2]\n",
    "    numax = 10**predictions[:, 3]\n",
    "    FeH = predictions[:, 4]  # FeH does not require log transformation\n",
    "\n",
    "    # Store results in the dictionary\n",
    "    predictions_dict[track_name] = {\n",
    "        'teff': teff[0],\n",
    "        'luminosity': luminosity[0],\n",
    "        'dnu': dnu[0],\n",
    "        'numax': numax[0],\n",
    "        'FeH': FeH[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected data for track01624:\n",
      "  age: [10018.17118]\n",
      "  massini: [0.845]\n",
      "  fehini: [-1.05361328]\n",
      "  alphamlt: [1.97304687]\n",
      "  yini: [0.27909668]\n",
      "  eta: [0.17797852]\n",
      "  alphafe: [0.2]\n",
      "  teff: 5029.081279\n",
      "  luminosity: 58.13098507540302\n",
      "  dnufit: 3.770210069783087\n",
      "  FeH: -1.0436679355001754\n",
      "  numax: 0.008890951555915818\n",
      "\n",
      "Selected data for track01975:\n",
      "  age: [12616.35002]\n",
      "  massini: [0.869]\n",
      "  fehini: [-0.70556641]\n",
      "  alphamlt: [1.61523438]\n",
      "  yini: [0.25497559]\n",
      "  eta: [0.15864258]\n",
      "  alphafe: [0.2]\n",
      "  teff: 4619.104165\n",
      "  luminosity: 41.33889352574103\n",
      "  dnufit: 3.85436674208666\n",
      "  FeH: -0.6940054574740909\n",
      "  numax: 0.009556647780866819\n",
      "\n",
      "Selected data for track08968:\n",
      "  age: [8840.709019]\n",
      "  massini: [0.854]\n",
      "  fehini: [-1.6260376]\n",
      "  alphamlt: [1.53208008]\n",
      "  yini: [0.27439148]\n",
      "  eta: [0.15650024]\n",
      "  alphafe: [0.2]\n",
      "  teff: 5005.701981\n",
      "  luminosity: 49.638260036401455\n",
      "  dnufit: 4.291318184571222\n",
      "  FeH: -1.6176067228259974\n",
      "  numax: 0.01036113645753787\n",
      "\n",
      "Selected data for track02584:\n",
      "  age: [10994.70683]\n",
      "  massini: [0.828]\n",
      "  fehini: [-1.51821289]\n",
      "  alphamlt: [1.97988281]\n",
      "  yini: [0.25938721]\n",
      "  eta: [0.1298584]\n",
      "  alphafe: [0.2]\n",
      "  teff: 5160.014377\n",
      "  luminosity: 57.55461547061882\n",
      "  dnufit: 4.073602338680234\n",
      "  FeH: -1.5102963267438847\n",
      "  numax: 0.00963656494125588\n",
      "\n",
      "Selected data for track04056:\n",
      "  age: [8621.133586]\n",
      "  massini: [0.837]\n",
      "  fehini: [-1.82006836]\n",
      "  alphamlt: [1.73027344]\n",
      "  yini: [0.28687256]\n",
      "  eta: [0.12062988]\n",
      "  alphafe: [0.2]\n",
      "  teff: 5140.341361\n",
      "  luminosity: 55.91856903276131\n",
      "  dnufit: 4.186189613317922\n",
      "  FeH: -1.8127530609253273\n",
      "  numax: 0.009898848999330651\n",
      "\n",
      "Selected data for track05847:\n",
      "  age: [11682.19675]\n",
      "  massini: [0.893]\n",
      "  fehini: [-0.45876465]\n",
      "  alphamlt: [2.06201172]\n",
      "  yini: [0.27800171]\n",
      "  eta: [0.14366455]\n",
      "  alphafe: [0.2]\n",
      "  teff: 4801.907216\n",
      "  luminosity: 46.777399661986486\n",
      "  dnufit: 3.9524335516026614\n",
      "  FeH: -0.446234204280798\n",
      "  numax: 0.00994922395659924\n",
      "\n",
      "Selected data for track05944:\n",
      "  age: [12296.74455]\n",
      "  massini: [0.816]\n",
      "  fehini: [-1.34821777]\n",
      "  alphamlt: [1.50107422]\n",
      "  yini: [0.25388061]\n",
      "  eta: [0.12432861]\n",
      "  alphafe: [0.2]\n",
      "  teff: 4873.842819\n",
      "  luminosity: 41.933170072802916\n",
      "  dnufit: 4.387724052822876\n",
      "  FeH: -1.339975328301087\n",
      "  numax: 0.010679177802501357\n",
      "\n",
      "Selected data for track06984:\n",
      "  age: [9283.666014]\n",
      "  massini: [0.873]\n",
      "  fehini: [-1.70861816]\n",
      "  alphamlt: [2.25830078]\n",
      "  yini: [0.25213501]\n",
      "  eta: [0.13026123]\n",
      "  alphafe: [0.2]\n",
      "  teff: 5288.697216\n",
      "  luminosity: 65.70164760790432\n",
      "  dnufit: 4.059402722294051\n",
      "  FeH: -1.7001414098285208\n",
      "  numax: 0.009703812690707577\n",
      "\n",
      "Selected data for track07032:\n",
      "  age: [10018.6003]\n",
      "  massini: [0.81]\n",
      "  fehini: [-1.74299316]\n",
      "  alphamlt: [1.54580078]\n",
      "  yini: [0.28260376]\n",
      "  eta: [0.17244873]\n",
      "  alphafe: [0.2]\n",
      "  teff: 5039.113635\n",
      "  luminosity: 48.82994274570983\n",
      "  dnufit: 4.312692530974965\n",
      "  FeH: -1.736156109063205\n",
      "  numax: 0.01021685546510529\n",
      "\n",
      "Selected data for track07783:\n",
      "  age: [12292.50694]\n",
      "  massini: [0.833]\n",
      "  fehini: [-0.81379395]\n",
      "  alphamlt: [2.17978516]\n",
      "  yini: [0.27200317]\n",
      "  eta: [0.18988037]\n",
      "  alphafe: [0.2]\n",
      "  teff: 5026.697731\n",
      "  luminosity: 54.25367319292772\n",
      "  dnufit: 3.907643809399119\n",
      "  FeH: -0.8035921921658777\n",
      "  numax: 0.009373104652118141\n"
     ]
    }
   ],
   "source": [
    "# Print selected data for all stars\n",
    "for track_name in track_names:\n",
    "    print(f\"\\nSelected data for {track_name}:\")\n",
    "    for key, value in star_data[track_name]['selected'].items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Min/Max values across all tracks:\n",
      "age: min = 8621.133586, max = 12616.35002\n",
      "massini: min = 0.81, max = 0.893\n",
      "fehini: min = -1.82006836, max = -0.458764648\n",
      "alphamlt: min = 1.50107422, max = 2.25830078\n",
      "yini: min = 0.25213501, max = 0.286872559\n",
      "eta: min = 0.120629883, max = 0.189880371\n",
      "alphafe: min = 0.2, max = 0.2\n",
      "teff: min = 4619.104165, max = 5288.697216\n",
      "luminosity: min = 41.33889352574103, max = 65.70164760790432\n",
      "dnufit: min = 3.770210069783087, max = 4.387724052822876\n",
      "FeH: min = -1.8127530609253273, max = -0.446234204280798\n",
      "numax: min = 0.008890951555915818, max = 0.010679177802501357\n"
     ]
    }
   ],
   "source": [
    "# List of 10 specific track names\n",
    "track_names = ['track01624', 'track01975', 'track08968', 'track02584', 'track04056',\n",
    "               'track05847', 'track05944', 'track06984', 'track07032', 'track07783']\n",
    "\n",
    "# Dictionary to store extracted data\n",
    "selected_data = {  \n",
    "    'age': [], 'massini': [], 'fehini': [], 'alphamlt': [], 'yini': [], 'eta': [], 'alphafe': [],\n",
    "    'teff': [], 'luminosity': [], 'dnufit': [], 'FeH': [], 'numax': []\n",
    "}\n",
    "\n",
    "# Open HDF5 file and extract data for each track\n",
    "with h5py.File(garstec_data, 'r') as hdf:\n",
    "    for track_name in track_names:\n",
    "        specific_track = hdf['grid']['tracks'][track_name]\n",
    "\n",
    "        # Select middle age index\n",
    "        index = len(specific_track['age']) // 2\n",
    "\n",
    "        # Append selected values to lists\n",
    "        selected_data['age'].append(specific_track['age'][index].item())\n",
    "        selected_data['massini'].append(specific_track['massini'][index].item())\n",
    "        selected_data['fehini'].append(specific_track['FeHini'][index].item())\n",
    "        selected_data['alphamlt'].append(specific_track['alphaMLT'][index].item())\n",
    "        selected_data['yini'].append(specific_track['yini'][index].item())\n",
    "        selected_data['eta'].append(specific_track['eta'][index].item())\n",
    "        selected_data['alphafe'].append(specific_track['alphaFe'][index].item())\n",
    "        selected_data['teff'].append(specific_track['Teff'][index].item())\n",
    "        selected_data['luminosity'].append(specific_track['LPhot'][index].item())\n",
    "        selected_data['dnufit'].append(specific_track['dnufit'][index].item())\n",
    "        selected_data['FeH'].append(specific_track['FeH'][index].item())\n",
    "        selected_data['numax'].append(specific_track['numax'][index].item())\n",
    "\n",
    "# Print min/max values for each parameter\n",
    "print(\"\\nMin/Max values across all tracks:\")\n",
    "for key, values in selected_data.items():\n",
    "    print(f\"{key}: min = {min(values)}, max = {max(values)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()\n",
    "weight = [jnp.asarray(param.numpy()) for name, param in state_dict.items() if \"weight\" in name]\n",
    "bias = [jnp.asarray(param.numpy()) for name, param in state_dict.items() if \"bias\" in name]\n",
    "\n",
    "\n",
    "def emulate(x):\n",
    "    if x.ndim == 1:\n",
    "        x = x[None, :]  # Convert to shape (1, features) for a single sample\n",
    "\n",
    "    # Hidden layers\n",
    "    for i, (w, b) in enumerate(zip(weight[:-1], bias[:-1])):\n",
    "        if x.shape[1] != w.shape[1]:\n",
    "            raise ValueError(f\"Shape mismatch in layer {i}: x.shape[1] ({x.shape[1]}) != w.shape[1] ({w.shape[1]})\")\n",
    "        #print(f\"Layer {i}: x shape: {x.shape}, w.T shape: {w.T.shape}, b shape: {b.shape}\")\n",
    "        x = jax.nn.relu(jnp.dot(x, w.T) + b)\n",
    "\n",
    "    # Final layer\n",
    "    if x.shape[1] != weight[-1].shape[1]:  # Corrected check\n",
    "        raise ValueError(f\"Final layer mismatch: x.shape[1] ({x.shape[1]}) != weight[-1].shape[1] ({weight[-1].shape[1]})\")\n",
    "    #print(f\"Final layer: x shape: {x.shape}, weight[-1].T shape: {weight[-1].T.shape}, bias[-1] shape: {bias[-1].shape}\")\n",
    "    x = jnp.dot(x, weight[-1].T) + bias[-1]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first teff value from each track and store it in an array\n",
    "teff_values = [star_data[track]['teff'][0] for track in track_names]\n",
    "lum_values = [star_data[track]['luminosity'][0] for track in track_names]\n",
    "dnufit_values = [star_data[track]['dnufit'][0] for track in track_names]\n",
    "FeH_values = [star_data[track]['FeH'][0] for track in track_names]\n",
    "numax_values = [star_data[track]['numax'][0] for track in track_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For observational data\n",
    "obs_err = {\n",
    "    'teff_err': [70],\n",
    "    'lum_err': [5],\n",
    "    'dnu_err': [0.1],\n",
    "    'feh_err': [0.1],\n",
    "    'numax_err': [0.5]\n",
    "}\n",
    "\n",
    "# Initialize lists to store observations\n",
    "teff_obs_list = []\n",
    "lum_obs_list = []\n",
    "dnu_obs_list = []\n",
    "feh_obs_list = []\n",
    "numax_obs_list = []\n",
    "\n",
    "for i in range(len(track_names)):\n",
    "    teff_obs = teff_values[i] + np.random.randn() * obs_err['teff_err'][0]\n",
    "    lum_obs = lum_values[i] + np.random.randn() * obs_err['lum_err'][0]\n",
    "    dnu_obs = dnufit_values[i] + np.random.randn() * obs_err['dnu_err'][0]\n",
    "    feh_obs = FeH_values[i] + np.random.randn() * obs_err['feh_err'][0]  # Fixed error term\n",
    "    numax_obs = numax_values[i] * sun_numax + np.random.randn() * obs_err['numax_err'][0]\n",
    "    \n",
    "    # Append to lists\n",
    "    teff_obs_list.append(teff_obs)\n",
    "    lum_obs_list.append(lum_obs)\n",
    "    dnu_obs_list.append(dnu_obs)\n",
    "    feh_obs_list.append(feh_obs)\n",
    "    numax_obs_list.append(numax_obs)\n",
    "\n",
    "# Create observation dictionary with all observations\n",
    "obs = {\n",
    "    'teff': teff_obs_list,\n",
    "    'lum': lum_obs_list,\n",
    "    'dnu': dnu_obs_list,\n",
    "    'feh': feh_obs_list,\n",
    "    'numax': numax_obs_list,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nteff_obs = teff_emu + np.random.randn() * obs_err['teff_err'][0]\\nlum_obs = lum_emu + np.random.randn() * obs_err['lum_err'][0]\\ndnu_obs = dnu_emu + np.random.randn() * obs_err['dnu_err'][0]\\nFeH_obs = FeH_emu + np.random.randn() * obs_err['lum_err'][0]\\nnumax_obs = numax_emu * sun_numax + np.random.randn() * obs_err['numax_err'][0]\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "teff_obs = teff_emu + np.random.randn() * obs_err['teff_err'][0]\n",
    "lum_obs = lum_emu + np.random.randn() * obs_err['lum_err'][0]\n",
    "dnu_obs = dnu_emu + np.random.randn() * obs_err['dnu_err'][0]\n",
    "FeH_obs = FeH_emu + np.random.randn() * obs_err['lum_err'][0]\n",
    "numax_obs = numax_emu * sun_numax + np.random.randn() * obs_err['numax_err'][0]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bmodel(obs=None):\n",
    "    with numpyro.plate(\"star\", 10):\n",
    "        # Define priors\n",
    "        massini_ = numpyro.deterministic(\"massini_\", 0.8 * numpyro.sample(\"massini_s\", dist.Beta(1.5, 3)) + 0.7)\n",
    "        tau_hat = numpyro.deterministic(\"tau_hat\", 4 * numpyro.sample(\"tau_hat_s\", dist.Beta(1.25, 2)) + 1) \n",
    "        \n",
    "        # Calculate values for joint prior\n",
    "        tau_ms = (2500) * (massini_**-3.15)\n",
    "        ages_ = numpyro.deterministic(\"ages_\", jnp.minimum(tau_hat * tau_ms, 20000))\n",
    "        \n",
    "        # Rest of priors\n",
    "        alphamlt_ = numpyro.deterministic(\"alphamlt_\", 0.8 * numpyro.sample(\"alphamlt_s\", dist.Beta(2, 2)) + 1.5)\n",
    "        yini_ = numpyro.deterministic(\"yini_\", 0.13 * numpyro.sample(\"yini_s\", dist.Beta(2, 2)) + 0.22)\n",
    "        eta_ = numpyro.deterministic(\"eta_\", 0.3 * numpyro.sample(\"eta_s\", dist.Beta(2, 2)))\n",
    "        alphafe_ = numpyro.deterministic(\"alphafe_\", 0.8 * numpyro.sample(\"alphafe_s\", dist.Beta(2, 2)) - 0.2)\n",
    "        fehini_ = numpyro.deterministic(\"fehini_\", 2.2 * numpyro.sample(\"fehini_s\", dist.Beta(2, 2)) - 2) \n",
    "        \n",
    "        # Prepare input features for PyTorch model\n",
    "        epsilon = 1e-10\n",
    "        log_vars_inputs = [ages_, massini_, alphamlt_, eta_, yini_] \n",
    "        log_transformed_inputs = [jnp.log10(jnp.maximum(var, epsilon)) for var in log_vars_inputs]\n",
    "        \n",
    "        # Stack features - this will have batch dimension from the plate\n",
    "        x = jnp.stack([*log_transformed_inputs, fehini_, alphafe_], axis=1)\n",
    "        \n",
    "        # Scale x manually - accounting for batch dimension\n",
    "        mean_x = jnp.array(scaler_X.mean_)\n",
    "        scale_x = jnp.array(scaler_X.scale_)\n",
    "        \n",
    "        # Add batch dimension to scaling parameters if needed\n",
    "        if x.ndim > mean_x.ndim:\n",
    "            # Expand dimensions to allow broadcasting across batch\n",
    "            mean_x = mean_x[None, :]  # Shape becomes (1, 7)\n",
    "            scale_x = scale_x[None, :]  # Shape becomes (1, 7)\n",
    "            \n",
    "        x_scaled = (x - mean_x) / scale_x\n",
    "        \n",
    "        # Emulate using PyTorch model\n",
    "        y_scaled = emulate(x_scaled)\n",
    "        \n",
    "        # De-scale y manually - also accounting for batch dimension\n",
    "        mean_y = jnp.array(scaler_y.mean_)\n",
    "        std_y = jnp.array(scaler_y.scale_)\n",
    "        \n",
    "        # Add batch dimension to y scaling parameters if needed\n",
    "        if y_scaled.ndim > mean_y.ndim:\n",
    "            mean_y = mean_y[None, :]\n",
    "            std_y = std_y[None, :]\n",
    "            \n",
    "        y = y_scaled * std_y + mean_y\n",
    "        \n",
    "        # Extract predictions\n",
    "        teff = numpyro.deterministic(\"teff\", jnp.power(10.0, y[..., 0])) \n",
    "        lum = numpyro.deterministic(\"lum\", jnp.power(10.0, y[..., 1])) \n",
    "        dnu = numpyro.deterministic(\"dnu\", jnp.power(10.0, y[..., 2]))\n",
    "        numax = numpyro.deterministic(\"numax\", jnp.power(10.0, y[..., 3])) * sun_numax\n",
    "        feh = numpyro.deterministic(\"feh\", y[..., 4])\n",
    "        \n",
    "    # Observational likelihoods\n",
    "    if obs is not None:\n",
    "        num_obs = len(obs['teff'])\n",
    "        \n",
    "        with numpyro.plate(\"observations\", num_obs):\n",
    "            # Convert observations to arrays\n",
    "            teff_obs = jnp.array(obs['teff'])\n",
    "            lum_obs = jnp.array(obs['lum'])\n",
    "            dnu_obs = jnp.array(obs['dnu'])\n",
    "            feh_obs = jnp.array(obs['feh'])\n",
    "            numax_obs = jnp.array(obs['numax'])\n",
    "            \n",
    "            # Sample observations\n",
    "            numpyro.sample(\"teff_obs\", dist.StudentT(5, teff, obs_err['teff_err'][0]), obs=teff_obs)\n",
    "            numpyro.sample(\"lum_obs\", dist.StudentT(5, lum, obs_err['lum_err'][0]), obs=lum_obs)\n",
    "            numpyro.sample(\"dnu_obs\", dist.StudentT(5, dnu, obs_err['dnu_err'][0]), obs=dnu_obs)\n",
    "            numpyro.sample(\"feh_obs\", dist.StudentT(5, feh, obs_err['feh_err'][0]), obs=feh_obs)\n",
    "            numpyro.sample(\"numax_obs\", dist.StudentT(5, numax, obs_err['numax_err'][0]), obs=numax_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_9104\\4120305975.py:6: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n",
      "  mcmc = MCMC(nuts, num_warmup=1000, num_samples=1000, num_chains=4) # between 1000 and 4000 for testing\n",
      "sample: 100%|██████████| 2000/2000 [18:52<00:00,  1.77it/s, 1023 steps of size 1.83e-04. acc. prob=0.95]\n",
      "sample: 100%|██████████| 2000/2000 [15:52<00:00,  2.10it/s, 1023 steps of size 2.97e-04. acc. prob=0.92]\n",
      "sample: 100%|██████████| 2000/2000 [14:26<00:00,  2.31it/s, 1023 steps of size 3.08e-04. acc. prob=0.91]\n",
      "sample: 100%|██████████| 2000/2000 [14:25<00:00,  2.31it/s, 1023 steps of size 2.36e-04. acc. prob=0.95]\n"
     ]
    }
   ],
   "source": [
    "from jax import random\n",
    "from numpyro.infer import Predictive\n",
    "from numpyro.infer.initialization import init_to_median\n",
    "\n",
    "nuts = NUTS(Bmodel, target_accept_prob=0.8, init_strategy=init_to_median, find_heuristic_step_size=True)\n",
    "mcmc = MCMC(nuts, num_warmup=1000, num_samples=1000, num_chains=4) # between 1000 and 4000 for testing \n",
    "rng = random.PRNGKey(0)\n",
    "rng, key = random.split(rng)\n",
    "\n",
    "mcmc.run(key, obs=obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ages_[0]</th>\n",
       "      <td>12175.662</td>\n",
       "      <td>1165.484</td>\n",
       "      <td>10100.956</td>\n",
       "      <td>14754.378</td>\n",
       "      <td>327.140</td>\n",
       "      <td>237.050</td>\n",
       "      <td>13.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ages_[1]</th>\n",
       "      <td>12009.333</td>\n",
       "      <td>3660.936</td>\n",
       "      <td>5510.877</td>\n",
       "      <td>16753.758</td>\n",
       "      <td>1694.791</td>\n",
       "      <td>1282.481</td>\n",
       "      <td>6.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ages_[2]</th>\n",
       "      <td>9428.991</td>\n",
       "      <td>1050.054</td>\n",
       "      <td>7781.836</td>\n",
       "      <td>11501.018</td>\n",
       "      <td>440.665</td>\n",
       "      <td>330.644</td>\n",
       "      <td>6.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ages_[3]</th>\n",
       "      <td>11347.223</td>\n",
       "      <td>1028.696</td>\n",
       "      <td>9476.368</td>\n",
       "      <td>13233.543</td>\n",
       "      <td>373.202</td>\n",
       "      <td>274.744</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ages_[4]</th>\n",
       "      <td>8791.427</td>\n",
       "      <td>1010.719</td>\n",
       "      <td>6850.489</td>\n",
       "      <td>10352.707</td>\n",
       "      <td>381.093</td>\n",
       "      <td>281.511</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yini_s[5]</th>\n",
       "      <td>0.419</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.068</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yini_s[6]</th>\n",
       "      <td>0.494</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.087</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yini_s[7]</th>\n",
       "      <td>0.337</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.044</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yini_s[8]</th>\n",
       "      <td>0.366</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.049</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yini_s[9]</th>\n",
       "      <td>0.483</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.081</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                mean        sd     hdi_3%    hdi_97%  mcse_mean   mcse_sd  \\\n",
       "ages_[0]   12175.662  1165.484  10100.956  14754.378    327.140   237.050   \n",
       "ages_[1]   12009.333  3660.936   5510.877  16753.758   1694.791  1282.481   \n",
       "ages_[2]    9428.991  1050.054   7781.836  11501.018    440.665   330.644   \n",
       "ages_[3]   11347.223  1028.696   9476.368  13233.543    373.202   274.744   \n",
       "ages_[4]    8791.427  1010.719   6850.489  10352.707    381.093   281.511   \n",
       "...              ...       ...        ...        ...        ...       ...   \n",
       "yini_s[5]      0.419     0.201      0.157      0.801      0.090     0.068   \n",
       "yini_s[6]      0.494     0.237      0.083      0.846      0.114     0.087   \n",
       "yini_s[7]      0.337     0.141      0.120      0.610      0.059     0.044   \n",
       "yini_s[8]      0.366     0.131      0.131      0.535      0.064     0.049   \n",
       "yini_s[9]      0.483     0.227      0.150      0.863      0.107     0.081   \n",
       "\n",
       "           ess_bulk  ess_tail  r_hat  \n",
       "ages_[0]       13.0      32.0   1.23  \n",
       "ages_[1]        6.0      24.0   1.86  \n",
       "ages_[2]        6.0      32.0   1.70  \n",
       "ages_[3]        7.0      15.0   1.55  \n",
       "ages_[4]        7.0      12.0   1.59  \n",
       "...             ...       ...    ...  \n",
       "yini_s[5]       6.0      12.0   1.92  \n",
       "yini_s[6]       5.0      16.0   2.74  \n",
       "yini_s[7]       6.0      20.0   1.69  \n",
       "yini_s[8]       5.0      13.0   2.93  \n",
       "yini_s[9]       5.0      17.0   2.46  \n",
       "\n",
       "[200 rows x 9 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import arviz as az\n",
    "\n",
    "trace = az.from_numpyro(mcmc)\n",
    "\n",
    "az.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcorner\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m corner\u001b[38;5;241m.\u001b[39mcorner(\u001b[43mtrace\u001b[49m, var_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mages_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmassini_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malphamlt_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meta_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myini_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfehini_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malphafe_\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trace' is not defined"
     ]
    }
   ],
   "source": [
    "import corner\n",
    "corner.corner(trace, var_names=['ages_', 'massini_', 'alphamlt_', 'eta_', 'yini_', 'fehini_', 'alphafe_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
