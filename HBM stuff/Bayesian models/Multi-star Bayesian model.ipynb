{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import jax\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS, Predictive\n",
    "import corner\n",
    "\n",
    "garstec_data = 'C:\\\\Users\\\\Dell\\\\Downloads\\\\Garstec_AS09_chiara.hdf5'\n",
    "\n",
    "sun_numax = 3090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:/Users/Dell/OneDrive - University of Birmingham/Documents/Y4 Project/y4project/HBM stuff/Bayesian models/best_model_v9-1-SS---epoch=8563-val_loss=0.00008340.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m  \u001b[38;5;66;03m# Number of input features\u001b[39;00m\n\u001b[0;32m     56\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Number of output features\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGarstecNet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m     )\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1581\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1500\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1579\u001b[0m \n\u001b[0;32m   1580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1581\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\core\\saving.py:63\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m map_location \u001b[38;5;241m=\u001b[39m map_location \u001b[38;5;129;01mor\u001b[39;00m _default_map_location\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[1;32m---> 63\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[0;32m     66\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m _pl_migrate_checkpoint(\n\u001b[0;32m     67\u001b[0m     checkpoint, checkpoint_path\u001b[38;5;241m=\u001b[39m(checkpoint_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(checkpoint_path, (\u001b[38;5;28mstr\u001b[39m, Path)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     68\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:59\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(path_or_url, map_location, weights_only)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload_state_dict_from_url(\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28mstr\u001b[39m(path_or_url),\n\u001b[0;32m     55\u001b[0m         map_location\u001b[38;5;241m=\u001b[39mmap_location,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m     56\u001b[0m         weights_only\u001b[38;5;241m=\u001b[39mweights_only,\n\u001b[0;32m     57\u001b[0m     )\n\u001b[0;32m     58\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m     61\u001b[0m         f,\n\u001b[0;32m     62\u001b[0m         map_location\u001b[38;5;241m=\u001b[39mmap_location,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m     63\u001b[0m         weights_only\u001b[38;5;241m=\u001b[39mweights_only,\n\u001b[0;32m     64\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\fsspec\\spec.py:1310\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[1;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1309\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[1;32m-> 1310\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1319\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[1;32mc:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\fsspec\\implementations\\local.py:200\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[1;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\fsspec\\implementations\\local.py:364\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[1;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[1;32m--> 364\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\fsspec\\implementations\\local.py:369\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m--> 369\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[0;32m    371\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:/Users/Dell/OneDrive - University of Birmingham/Documents/Y4 Project/y4project/HBM stuff/Bayesian models/best_model_v9-1-SS---epoch=8563-val_loss=0.00008340.ckpt'"
     ]
    }
   ],
   "source": [
    "class GarstecNet(LightningModule):\n",
    "    def __init__(self, input_dim, output_dim, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = lr\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),  # First layer maps input_dim to 256 neurons\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 256),  # 2\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 128),  # 3\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 64),  # 4\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32),  # 5\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, output_dim)  # Output layer\n",
    "        )\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=25, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1\n",
    "            },\n",
    "        }\n",
    "    \n",
    "ckpt_path = \"best_model_v9-1-SS---epoch=8563-val_loss=0.00008340.ckpt\" \n",
    "input_dim = 7  # Number of input features\n",
    "output_dim = 5  # Number of output features\n",
    "model = GarstecNet.load_from_checkpoint(ckpt_path, input_dim=input_dim, output_dim=output_dim)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "scaler_X = joblib.load('scalerXV9.pkl')\n",
    "scaler_y = joblib.load('scalerYV9.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "specific_track_name = 'track02584'\n",
    "\n",
    "# Retrieve all required inputs for track: 07298\n",
    "with h5py.File(garstec_data, 'r') as hdf:\n",
    "    specific_track = hdf['grid']['tracks'][specific_track_name]\n",
    "\n",
    "    age_07298 = specific_track['age'][:].reshape(-1, 1)\n",
    "    massini_07298 = specific_track['massini'][:].reshape(-1, 1)\n",
    "    fehini_07298 = specific_track['FeHini'][:].reshape(-1, 1)\n",
    "    alphamlt_07298 = specific_track['alphaMLT'][:].reshape(-1, 1)\n",
    "    yini_07298 = specific_track['yini'][:].reshape(-1, 1)\n",
    "    eta_07298 = specific_track['eta'][:].reshape(-1, 1)\n",
    "    alphafe_07298 = specific_track['alphaFe'][:].reshape(-1, 1)\n",
    "\n",
    "    # Retrieve actual values for plotting\n",
    "    teff_07298 = specific_track['Teff'][:]\n",
    "    luminosity_07298 = specific_track['LPhot'][:]\n",
    "    dnufit_07298 = specific_track['dnufit'][:]\n",
    "    FeH_07298 = specific_track['FeH'][:]\n",
    "    numax_07298 = specific_track['numax'][:]\n",
    "\n",
    "# Using a single age point from the middle of the ages:\n",
    "\n",
    "index_07298 = round(len(age_07298)/2)\n",
    "\n",
    "age_07298_ = age_07298[index_07298]\n",
    "massini_07298_ = massini_07298[index_07298]\n",
    "fehini_07298_ = fehini_07298[index_07298]\n",
    "alphamlt_07298_ = alphamlt_07298[index_07298]\n",
    "yini_07298_ = yini_07298[index_07298]\n",
    "eta_07298_ = eta_07298[index_07298]\n",
    "alphafe_07298_ = alphafe_07298[index_07298]\n",
    "\n",
    "\n",
    "teff_07298_ = teff_07298[index_07298]\n",
    "luminosity_07298_ = luminosity_07298[index_07298]\n",
    "dnufit_07298_ = dnufit_07298[index_07298]\n",
    "FeH_07298_ = FeH_07298[index_07298]\n",
    "numax_07298_ = numax_07298[index_07298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-10\n",
    "\n",
    "log10_inputs = [age_07298, massini_07298, alphamlt_07298, eta_07298, yini_07298] \n",
    "log10_transformed_inputs = [np.log10(np.maximum(data, epsilon)) for data in log10_inputs]\n",
    "\n",
    "# Combine log-transformed inputs with raw `fehini` and `alpha fe`\n",
    "features_real = np.hstack(log10_transformed_inputs + [fehini_07298, alphafe_07298])\n",
    "\n",
    "features_numpy_real = np.array(features_real)  # Convert to numpy for scaling\n",
    "\n",
    "features_scaled_real = scaler_X.transform(features_numpy_real)  # Apply scaler\n",
    "\n",
    "# Convert scaled inputs to PyTorch tensor\n",
    "\n",
    "features_tensor_real = torch.FloatTensor(features_scaled_real)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(features_tensor_real).numpy()  # Make predictions\n",
    "    predictions = scaler_y.inverse_transform(predictions)  # Inverse transform\n",
    "\n",
    "# Extract predicted `Teff` and `Luminosity`\n",
    "teff = 10**predictions[:, 0]  # Inverse log10 transformation\n",
    "luminosity = 10**predictions[:, 1]\n",
    "dnu = 10**predictions[:, 2]\n",
    "numax = 10**predictions[:, 3]\n",
    "FeH = 10**predictions[:, 4]\n",
    "\n",
    "teff_emu = teff[index_07298]\n",
    "lum_emu = luminosity[index_07298]\n",
    "dnu_emu = dnu[index_07298]\n",
    "numax_emu = numax[index_07298]\n",
    "FeH_emu = FeH[index_07298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()\n",
    "weight = [jnp.asarray(param.numpy()) for name, param in state_dict.items() if \"weight\" in name]\n",
    "bias = [jnp.asarray(param.numpy()) for name, param in state_dict.items() if \"bias\" in name]\n",
    "\n",
    "\n",
    "def emulate(x):\n",
    "    if x.ndim == 1:\n",
    "        x = x[None, :]  # Convert to shape (1, features) for a single sample\n",
    "\n",
    "    # Hidden layers\n",
    "    for i, (w, b) in enumerate(zip(weight[:-1], bias[:-1])):\n",
    "        if x.shape[1] != w.shape[1]:\n",
    "            raise ValueError(f\"Shape mismatch in layer {i}: x.shape[1] ({x.shape[1]}) != w.shape[1] ({w.shape[1]})\")\n",
    "        #print(f\"Layer {i}: x shape: {x.shape}, w.T shape: {w.T.shape}, b shape: {b.shape}\")\n",
    "        x = jax.nn.relu(jnp.dot(x, w.T) + b)\n",
    "\n",
    "    # Final layer\n",
    "    if x.shape[1] != weight[-1].shape[1]:  # Corrected check\n",
    "        raise ValueError(f\"Final layer mismatch: x.shape[1] ({x.shape[1]}) != weight[-1].shape[1] ({weight[-1].shape[1]})\")\n",
    "    #print(f\"Final layer: x shape: {x.shape}, weight[-1].T shape: {weight[-1].T.shape}, bias[-1] shape: {bias[-1].shape}\")\n",
    "    x = jnp.dot(x, weight[-1].T) + bias[-1]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running EMU this time with nn values:\n",
    "\n",
    "obs_err = {\n",
    "    'teff_err': [70],\n",
    "    'lum_err': [5],\n",
    "    'dnu_err': [0.1],\n",
    "    'feh_err': [0.1],\n",
    "    'numax_err': [0.5]\n",
    "}\n",
    "\n",
    "teff_obs = teff_07298_ + np.random.randn() * obs_err['teff_err'][0]\n",
    "lum_obs = luminosity_07298_ + np.random.randn() * obs_err['lum_err'][0]\n",
    "dnu_obs = dnufit_07298_ + np.random.randn() * obs_err['dnu_err'][0]\n",
    "FeH_obs = FeH_07298_ + np.random.randn() * obs_err['lum_err'][0]\n",
    "numax_obs = numax_07298_ * sun_numax + np.random.randn() * obs_err['numax_err'][0]\n",
    "\"\"\"\n",
    "teff_obs = teff_emu + np.random.randn() * obs_err['teff_err'][0]\n",
    "lum_obs = lum_emu + np.random.randn() * obs_err['lum_err'][0]\n",
    "dnu_obs = dnu_emu + np.random.randn() * obs_err['dnu_err'][0]\n",
    "FeH_obs = FeH_emu + np.random.randn() * obs_err['lum_err'][0]\n",
    "numax_obs = numax_emu * sun_numax + np.random.randn() * obs_err['numax_err'][0]\n",
    "\"\"\"\n",
    "obs = {\n",
    "    'teff': [teff_obs],\n",
    "    'lum': [lum_obs],\n",
    "    'dnu': [dnu_obs],\n",
    "    'feh': [FeH_obs],\n",
    "    'numax': [numax_obs],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bmodel(obs=None):\n",
    "    # Define priors\n",
    "\n",
    "    massini_ = numpyro.deterministic(\"massini_\", 0.8 * numpyro.sample(\"massini_s\", dist.Beta(1.5, 3)) + 0.7)\n",
    "    tau_hat = numpyro.deterministic(\"tau_hat\", 4 * numpyro.sample(\"tau_hat_s\", dist.Beta(1.25, 2)) + 1) \n",
    "\n",
    "    # Calculate values for joint prior: \n",
    "    tau_ms = (2500) * (massini_**-3.15)\n",
    "    ages_ = numpyro.deterministic(\"ages_\", jnp.minimum(tau_hat * tau_ms, 20000)) # jnp.min to avoid values outside of grid\n",
    "\n",
    "    # Rest of priors:\n",
    "    alphamlt_ = numpyro.deterministic(\"alphamlt_\", 0.8 * numpyro.sample(\"alphamlt_s\", dist.Beta(2, 2)) + 1.5) # 0.8, 1.5\n",
    "    yini_ = numpyro.deterministic(\"yini_\", 0.13 * numpyro.sample(\"yini_s\", dist.Beta(2, 2)) + 0.22)\n",
    "    eta_ = numpyro.deterministic(\"eta_\", 0.3 * numpyro.sample(\"eta_s\", dist.Beta(2, 2)))\n",
    "    alphafe_ = numpyro.deterministic(\"alphafe_\", 0.8 * numpyro.sample(\"alphafe_s\", dist.Beta(2, 2)) - 0.2) # 0.8, -0.2\n",
    "    fehini_ = numpyro.deterministic(\"fehini_\", 2.2 * numpyro.sample(\"fehini_s\", dist.Beta(2, 2)) - 2) \n",
    "\n",
    "    # Prepare input features for PyTorch model\n",
    "    epsilon = 1e-10\n",
    "    log_vars_inputs = [ages_, massini_, alphamlt_, eta_, yini_] \n",
    "    log_transformed_inputs = [jnp.log10(jnp.maximum(var, epsilon)) for var in log_vars_inputs]\n",
    "    x = jnp.hstack(log_transformed_inputs + [fehini_, alphafe_])\n",
    "\n",
    "\n",
    "    # Scale x manually\n",
    "    mean_x = scaler_X.mean_\n",
    "    scale_x = scaler_X.scale_  \n",
    "    x_scaled = (x - mean_x) / scale_x\n",
    "\n",
    "    # Emulate using PyTorch model\n",
    "    y_scaled = emulate(x_scaled)\n",
    "    \n",
    "    # De-scale y manually \n",
    "    mean_y = jnp.array(scaler_y.mean_)  \n",
    "    std_y = jnp.array(scaler_y.scale_)\n",
    "    y = y_scaled * std_y + mean_y\n",
    "\n",
    "    # Extract predictions\n",
    "    teff = numpyro.deterministic(\"teff\", jnp.power(10.0, y[..., 0])) \n",
    "    lum = numpyro.deterministic(\"lum\", jnp.power(10.0, y[..., 1])) \n",
    "    dnu = numpyro.deterministic(\"dnu\", jnp.power(10.0, y[..., 2]))\n",
    "    numax = numpyro.deterministic(\"numax\", jnp.power(10.0, y[..., 3])) * sun_numax\n",
    "    feh = numpyro.deterministic(\"feh\", y[..., 4])\n",
    "\n",
    "    \n",
    "    # Observational likelihoods\n",
    "    if obs is not None:\n",
    "        numpyro.sample(\"teff_obs\", dist.StudentT(5, teff, obs_err['teff_err'][0]), obs=obs['teff'][0])\n",
    "        numpyro.sample(\"lum_obs\", dist.StudentT(5, lum, obs_err['lum_err'][0]), obs=obs['lum'][0])\n",
    "        numpyro.sample(\"dnu_obs\", dist.StudentT(5, dnu, obs_err['dnu_err'][0]), obs=obs['dnu'][0])\n",
    "        numpyro.sample(\"feh_obs\", dist.StudentT(5, feh, obs_err['feh_err'][0]), obs=obs['feh'][0])\n",
    "        numpyro.sample(\"numax_obs\", dist.StudentT(5, numax, obs_err['numax_err'][0]), obs=obs['numax'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "from numpyro.infer import Predictive\n",
    "from numpyro.infer.initialization import init_to_median\n",
    "\n",
    "nuts = NUTS(Bmodel, target_accept_prob=0.8, init_strategy=init_to_median, find_heuristic_step_size=True)\n",
    "mcmc = MCMC(nuts, num_warmup=8000, num_samples=8000, num_chains=4) # between 1000 and 4000 for testing \n",
    "rng = random.PRNGKey(0)\n",
    "rng, key = random.split(rng)\n",
    "\n",
    "mcmc.run(key, obs=obs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
