digraph {
	graph [size="12.45,12.45"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1692684451696 [label="
 (1, 5)" fillcolor=darkolivegreen1]
	1693920057648 [label=AddmmBackward0]
	1693930807232 -> 1693920057648
	1693865701008 [label="model.10.bias
 (5)" fillcolor=lightblue]
	1693865701008 -> 1693930807232
	1693930807232 [label=AccumulateGrad]
	1693930810736 -> 1693920057648
	1693930810736 [label=ReluBackward0]
	1693929858784 -> 1693930810736
	1693929858784 [label=AddmmBackward0]
	1693883698864 -> 1693929858784
	1693865700848 [label="model.8.bias
 (256)" fillcolor=lightblue]
	1693865700848 -> 1693883698864
	1693883698864 [label=AccumulateGrad]
	1693931470320 -> 1693929858784
	1693931470320 [label=ReluBackward0]
	1693931470416 -> 1693931470320
	1693931470416 [label=AddmmBackward0]
	1693930851968 -> 1693931470416
	1693865700688 [label="model.6.bias
 (256)" fillcolor=lightblue]
	1693865700688 -> 1693930851968
	1693930851968 [label=AccumulateGrad]
	1693930860416 -> 1693931470416
	1693930860416 [label=ReluBackward0]
	1693930730592 -> 1693930860416
	1693930730592 [label=AddmmBackward0]
	1693930732752 -> 1693930730592
	1693865700528 [label="model.4.bias
 (256)" fillcolor=lightblue]
	1693865700528 -> 1693930732752
	1693930732752 [label=AccumulateGrad]
	1693930730112 -> 1693930730592
	1693930730112 [label=ReluBackward0]
	1693930907408 -> 1693930730112
	1693930907408 [label=AddmmBackward0]
	1693930901264 -> 1693930907408
	1693865690848 [label="model.2.bias
 (256)" fillcolor=lightblue]
	1693865690848 -> 1693930901264
	1693930901264 [label=AccumulateGrad]
	1693931815024 -> 1693930907408
	1693931815024 [label=ReluBackward0]
	1693931815168 -> 1693931815024
	1693931815168 [label=AddmmBackward0]
	1693931815312 -> 1693931815168
	1692747172832 [label="model.0.bias
 (256)" fillcolor=lightblue]
	1692747172832 -> 1693931815312
	1693931815312 [label=AccumulateGrad]
	1693931815264 -> 1693931815168
	1693931815264 [label=TBackward0]
	1693930728720 -> 1693931815264
	1692752197376 [label="model.0.weight
 (256, 7)" fillcolor=lightblue]
	1692752197376 -> 1693930728720
	1693930728720 [label=AccumulateGrad]
	1693931814976 -> 1693930907408
	1693931814976 [label=TBackward0]
	1693930855952 -> 1693931814976
	1692776282976 [label="model.2.weight
 (256, 256)" fillcolor=lightblue]
	1692776282976 -> 1693930855952
	1693930855952 [label=AccumulateGrad]
	1693930911968 -> 1693930730592
	1693930911968 [label=TBackward0]
	1693930902560 -> 1693930911968
	1693865700288 [label="model.4.weight
 (256, 256)" fillcolor=lightblue]
	1693865700288 -> 1693930902560
	1693930902560 [label=AccumulateGrad]
	1693930861568 -> 1693931470416
	1693930861568 [label=TBackward0]
	1693930904384 -> 1693930861568
	1693865700608 [label="model.6.weight
 (256, 256)" fillcolor=lightblue]
	1693865700608 -> 1693930904384
	1693930904384 [label=AccumulateGrad]
	1693931470704 -> 1693929858784
	1693931470704 [label=TBackward0]
	1693930722480 -> 1693931470704
	1693865700768 [label="model.8.weight
 (256, 256)" fillcolor=lightblue]
	1693865700768 -> 1693930722480
	1693930722480 [label=AccumulateGrad]
	1693930431840 -> 1693920057648
	1693930431840 [label=TBackward0]
	1693931470608 -> 1693930431840
	1693865700928 [label="model.10.weight
 (5, 256)" fillcolor=lightblue]
	1693865700928 -> 1693931470608
	1693931470608 [label=AccumulateGrad]
	1693920057648 -> 1692684451696
}
