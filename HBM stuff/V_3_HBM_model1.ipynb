{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro\n",
    "import numpy as np \n",
    "\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS, Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "garstec_data = 'C:\\\\Users\\\\Dell\\\\Downloads\\\\Garstec_AS09_chiara.hdf5'\n",
    "\n",
    "# 7 Inputs\n",
    "ages = []\n",
    "massini = []\n",
    "fehini = []\n",
    "alphamlt = []\n",
    "yini = []\n",
    "eta = []\n",
    "alphafe = []\n",
    "\n",
    "# 8 Outputs\n",
    "teff = []\n",
    "luminosity = []\n",
    "dnufit = []\n",
    "FeH = []\n",
    "G_GAIA = []\n",
    "massfin = []\n",
    "numax = []\n",
    "MeH = []\n",
    "\n",
    "# Open the hdf5 file (read-only mode)\n",
    "with h5py.File(garstec_data, 'r') as hdf:\n",
    "\n",
    "    grid = hdf['grid']\n",
    "    tracks = grid['tracks']\n",
    "\n",
    "    # Get a list of track names and shuffle for random sampling\n",
    "    track_names = list(tracks.keys())\n",
    "    random.seed(1)\n",
    "    random.shuffle(track_names)\n",
    "\n",
    "    # Choose a subset of tracks to process\n",
    "    num_tracks = 1000  # Set the number of tracks to process\n",
    "    selected_tracks = track_names[:num_tracks]\n",
    "\n",
    "    for track_name in selected_tracks:  # Iterate over the selected track names\n",
    "        track = tracks[track_name]\n",
    "        \n",
    "        # Inputs\n",
    "        ages.append(track['age'][:])\n",
    "        massini.append(track['massini'][:])\n",
    "        fehini.append(track['FeHini'][:])\n",
    "        alphamlt.append(track['alphaMLT'][:])\n",
    "        yini.append(track['yini'][:])\n",
    "        eta.append(track['eta'][:])\n",
    "        alphafe.append(track['alphaFe'][:])\n",
    "\n",
    "        # Outputs\n",
    "        teff.append(track['Teff'][:])\n",
    "        luminosity.append(track['LPhot'][:])\n",
    "        dnufit.append(track['dnufit'][:])\n",
    "        FeH.append(track['FeH'][:])\n",
    "        G_GAIA.append(track['G_GAIA'][:])\n",
    "        massfin.append(track['massfin'][:])\n",
    "        numax.append(track['numax'][:])\n",
    "        MeH.append(track['MeH'][:])\n",
    "\n",
    "# Convert lists to numpy arrays and concatenate them (make one big list)\n",
    "\n",
    "# Define a small constant to avoid log10(0)\n",
    "epsilon = 1e-10\n",
    "\n",
    "# Features requiring log10 transformation\n",
    "log10_vars_inputs = [ages, massini, alphamlt, eta, alphafe]\n",
    "\n",
    "# Transform log10 variables\n",
    "log10_transformed_inputs = [np.log10(np.maximum(np.concatenate(var).reshape(-1, 1), epsilon)) for var in log10_vars_inputs]\n",
    "\n",
    "# Concatenate all inputs, including raw `fehini` and `yini`\n",
    "inputs = np.hstack(log10_transformed_inputs + [np.concatenate(fehini).reshape(-1, 1), np.concatenate(yini).reshape(-1, 1)])\n",
    "\n",
    "# Features requiring log10 transformation (strictly positive outputs)\n",
    "log10_vars_outputs = [teff, luminosity, dnufit, G_GAIA, massfin, numax]\n",
    "\n",
    "# Transform log10 variables\n",
    "log10_transformed_outputs = [np.log10(np.maximum(np.concatenate(var).reshape(-1, 1), epsilon)) for var in log10_vars_outputs]\n",
    "\n",
    "# Combine transformed log10 outputs with raw FeH and MeH\n",
    "# FeH and MeH are not transformed, concatenated directly\n",
    "outputs = np.hstack(log10_transformed_outputs + [np.concatenate(FeH).reshape(-1, 1), np.concatenate(MeH).reshape(-1, 1)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.2, random_state=1)\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "y_test = scaler_y.transform(y_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_34276\\3765155684.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('garstec_model_V3_state.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network\n",
    "class GarstecNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GarstecNet, self).__init__()\n",
    "        self.dense1 = nn.Linear(7, 64)   # Input layer\n",
    "        self.dense2 = nn.Linear(64, 64)\n",
    "        self.dense3 = nn.Linear(64, 64)  \n",
    "        self.dense4 = nn.Linear(64, 64)\n",
    "        self.dense5 = nn.Linear(64, 64)\n",
    "        self.dense6 = nn.Linear(64, 8)    # Output layer\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.dense1(x))\n",
    "        x = torch.relu(self.dense2(x))\n",
    "        x = torch.relu(self.dense3(x))  \n",
    "        x = torch.relu(self.dense4(x))\n",
    "        x = torch.relu(self.dense5(x))\n",
    "        x = self.dense6(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# Load the pre-trained model weights\n",
    "model = GarstecNet()\n",
    "model.load_state_dict(torch.load('garstec_model_V3_state.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.9806\n",
      "Mean Absolute Error: 0.0856\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor)\n",
    "    predictions = scaler_y.inverse_transform(predictions.numpy())\n",
    "    y_test_actual = scaler_y.inverse_transform(y_test_tensor.numpy())\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    r2 = r2_score(y_test_actual, predictions)\n",
    "    mae = mean_absolute_error(y_test_actual, predictions)\n",
    "\n",
    "    print(f'R^2 Score: {r2:.4f}')\n",
    "    print(f'Mean Absolute Error: {mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4932.64058133 4652.25822784 4891.95228153 5531.54853543 5091.07530194]\n",
      "[ 11.13728656  95.38456881   8.33429052  12.52324302 107.52646291]\n",
      "[11.68068749  2.61558766 16.65392518 16.70058139  2.85426828]\n",
      "[2.19656126e+00 1.28975863e-07 1.73269807e+00 1.46599437e+00\n",
      " 1.03831424e-10]\n",
      "[0.73441414 1.3223209  0.97818339 0.90089512 1.10754626]\n",
      "[0.03799601 0.00650092 0.06558518 0.06217756 0.00663714]\n",
      "[0.2827621  0.31256809 0.31589133 0.12381988 0.01711447]\n",
      "[0.31501224 0.31794541 0.24498929 0.16543927 0.05322677]\n"
     ]
    }
   ],
   "source": [
    "# Fixed number of stars\n",
    "n_stars = 5\n",
    "\n",
    "# Dictionary to store predictions\n",
    "predictions = {\n",
    "    'Teff': [],\n",
    "    'Luminosity': [],\n",
    "    'Dnufit': [],\n",
    "    'FeH': [],\n",
    "    'G_GAIA': [],\n",
    "    'Massfin': [],\n",
    "    'Numax': [],\n",
    "    'MeH': []\n",
    "}\n",
    "\n",
    "with h5py.File(garstec_data, 'r') as hdf:\n",
    "    for _ in range(n_stars):\n",
    "        # Select a random track\n",
    "        specific_track_name = np.random.choice(selected_tracks)\n",
    "        specific_track = hdf['grid']['tracks'][specific_track_name]\n",
    "        \n",
    "        # Randomly sample a single data point from the track\n",
    "        # This is because with too many points the model couldnt \"walk\" far enough so simplified it while keeping the\n",
    "        # cluster of stars property of the problem.\n",
    "        num_points = len(specific_track['age'])\n",
    "        idx = np.random.randint(0, num_points)  # Random index\n",
    "        \n",
    "        \n",
    "        # Retrieve features for the sampled point\n",
    "        ages = specific_track['age'][idx].reshape(-1, 1)\n",
    "        massini = specific_track['massini'][idx].reshape(-1, 1)\n",
    "        fehini = specific_track['FeHini'][idx].reshape(-1, 1)\n",
    "        alphamlt = specific_track['alphaMLT'][idx].reshape(-1, 1)\n",
    "        yini = specific_track['yini'][idx].reshape(-1, 1)\n",
    "        eta = specific_track['eta'][idx].reshape(-1, 1)\n",
    "        alphafe = specific_track['alphaFe'][idx].reshape(-1, 1)\n",
    "        \n",
    "        # Combine features into a single array\n",
    "        epsilon = 1e-10\n",
    "        log10_vars_inputs = [ages, massini, alphamlt, eta, alphafe]\n",
    "        log10_transformed_inputs = [np.log10(np.maximum(var, epsilon)) for var in log10_vars_inputs]\n",
    "        all_features = np.hstack(log10_transformed_inputs + [fehini, yini])\n",
    "\n",
    "        # Scale features\n",
    "        all_features_scaled = scaler_X.transform(all_features)\n",
    "        all_features_tensor = torch.FloatTensor(all_features_scaled)\n",
    "        \n",
    "        # Make predictions\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions_specific = model(all_features_tensor).numpy()\n",
    "            predictions_specific = scaler_y.inverse_transform(predictions_specific)\n",
    "\n",
    "\n",
    "        # Extract and store predictions for this star\n",
    "        predictions['Teff'].append(10**predictions_specific[0, 0])\n",
    "        predictions['Luminosity'].append(10**predictions_specific[0, 1])\n",
    "        predictions['Dnufit'].append(10**predictions_specific[0, 2])\n",
    "        predictions['FeH'].append(10**predictions_specific[0, 3])\n",
    "        predictions['G_GAIA'].append(10**predictions_specific[0, 4])\n",
    "        predictions['Massfin'].append(10**predictions_specific[0, 5])\n",
    "        predictions['Numax'].append(10**predictions_specific[0, 6])\n",
    "        predictions['MeH'].append(10**predictions_specific[0, 7])\n",
    "\n",
    "teff_obs = np.array(predictions['Teff'])\n",
    "luminosity_obs = np.array(predictions['Luminosity'])\n",
    "dnufit_obs = np.array(predictions['Dnufit'])\n",
    "FeH_obs = np.array(predictions['FeH'])\n",
    "G_GAIA_obs = np.array(predictions['G_GAIA'])\n",
    "massfin_obs = np.array(predictions['Massfin'])\n",
    "numax_obs = np.array(predictions['Numax'])\n",
    "MeH_obs = np.array(predictions['MeH'])\n",
    "\n",
    "# Checking data points for debugging\n",
    "print(teff_obs)\n",
    "print(luminosity_obs)\n",
    "print(dnufit_obs)\n",
    "print(FeH_obs)\n",
    "print(G_GAIA_obs)\n",
    "print(massfin_obs)\n",
    "print(numax_obs)\n",
    "print(MeH_obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_model(teff_obs, luminosity_obs, dnufit_obs, FeH_obs, \n",
    "                       G_GAIA_obs, massfin_obs, numax_obs, MeH_obs):\n",
    "    # Hyperpriors for group-level parameters\n",
    "    # Picked based on a brief look at the data in order to get the model working, more thought will have to be\n",
    "    # given to these, especially the smaller ones such as FeH etc\n",
    "    mean_teff = numpyro.sample('mean_teff', dist.Normal(6000, 500))\n",
    "    sigma_teff = numpyro.sample('sigma_teff', dist.HalfNormal(200))\n",
    "\n",
    "    mean_luminosity = numpyro.sample('mean_luminosity', dist.Normal(80, 20))\n",
    "    sigma_luminosity = numpyro.sample('sigma_luminosity', dist.HalfNormal(10))\n",
    "\n",
    "    mean_dnufit = numpyro.sample('mean_dnufit', dist.Normal(20, 5))\n",
    "    sigma_dnufit = numpyro.sample('sigma_dnufit', dist.HalfNormal(2))\n",
    "\n",
    "    mean_FeH = numpyro.sample('mean_FeH', dist.Normal(0, 0.2))\n",
    "    sigma_FeH = numpyro.sample('sigma_FeH', dist.HalfNormal(0.1))\n",
    "\n",
    "    mean_G_GAIA = numpyro.sample('mean_G_GAIA', dist.Normal(0.8, 0.05))\n",
    "    sigma_G_GAIA = numpyro.sample('sigma_G_GAIA', dist.HalfNormal(0.02))\n",
    "\n",
    "    mean_massfin = numpyro.sample('mean_massfin', dist.Normal(0.1, 0.05))\n",
    "    sigma_massfin = numpyro.sample('sigma_massfin', dist.HalfNormal(0.02))\n",
    "\n",
    "    mean_numax = numpyro.sample('mean_numax', dist.Normal(0.023, 0.002))\n",
    "    sigma_numax = numpyro.sample('sigma_numax', dist.HalfNormal(0.001))\n",
    "\n",
    "    mean_MeH = numpyro.sample('mean_MeH', dist.Normal(0.025, 0.1))\n",
    "    sigma_MeH = numpyro.sample('sigma_MeH', dist.HalfNormal(0.05))\n",
    "\n",
    "    # Star-level distributions \n",
    "    # Sampling the stars based off of the hyper priors above, using the numpyro plate feature\n",
    "    with numpyro.plate('stars', n_stars):\n",
    "        teff = numpyro.sample('teff', dist.Normal(mean_teff, sigma_teff))\n",
    "        luminosity = numpyro.sample('luminosity', dist.Normal(mean_luminosity, sigma_luminosity))\n",
    "        dnufit = numpyro.sample('dnufit', dist.Normal(mean_dnufit, sigma_dnufit))\n",
    "        FeH = numpyro.sample('FeH', dist.Normal(mean_FeH, sigma_FeH))\n",
    "        G_GAIA = numpyro.sample('G_GAIA', dist.Normal(mean_G_GAIA, sigma_G_GAIA))\n",
    "        massfin = numpyro.sample('massfin', dist.Normal(mean_massfin, sigma_massfin))\n",
    "        numax = numpyro.sample('numax', dist.Normal(mean_numax, sigma_numax))\n",
    "        MeH = numpyro.sample('MeH', dist.Normal(mean_MeH, sigma_MeH))\n",
    "\n",
    "        # Observation noise\n",
    "        obs_sigma = 5.0  # Picked as a starting point but more thought needs to be given\n",
    "        numpyro.sample('obs_teff', dist.Normal(teff, obs_sigma), obs=teff_obs)\n",
    "        numpyro.sample('obs_luminosity', dist.Normal(luminosity, obs_sigma), obs=luminosity_obs)\n",
    "        numpyro.sample('obs_dnufit', dist.Normal(dnufit, obs_sigma), obs=dnufit_obs)\n",
    "        numpyro.sample('obs_FeH', dist.Normal(FeH, obs_sigma), obs=FeH_obs)\n",
    "        numpyro.sample('obs_G_GAIA', dist.Normal(G_GAIA, obs_sigma), obs=G_GAIA_obs)\n",
    "        numpyro.sample('obs_massfin', dist.Normal(massfin, obs_sigma), obs=massfin_obs)\n",
    "        numpyro.sample('obs_numax', dist.Normal(numax, obs_sigma), obs=numax_obs)\n",
    "        numpyro.sample('obs_MeH', dist.Normal(MeH, obs_sigma), obs=MeH_obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_34276\\2889296754.py:6: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n",
      "  hbm_mcmc = MCMC(nuts_kernel, num_samples=8000, num_warmup=8000, num_chains=2, thinning=4)\n",
      "sample: 100%|██████████| 16000/16000 [06:33<00:00, 40.62it/s, 127 steps of size 3.26e-02. acc. prob=0.80] \n",
      "sample: 100%|██████████| 16000/16000 [06:09<00:00, 43.32it/s, 255 steps of size 2.57e-02. acc. prob=0.68] \n"
     ]
    }
   ],
   "source": [
    "from jax import random\n",
    "from numpyro.infer import NUTS, MCMC\n",
    "\n",
    "# Set up the NUTS kernel and MCMC\n",
    "nuts_kernel = NUTS(hierarchical_model)\n",
    "hbm_mcmc = MCMC(nuts_kernel, num_samples=8000, num_warmup=8000, num_chains=2, thinning=4)\n",
    "\n",
    "# Random seed\n",
    "rng_key = random.PRNGKey(0)\n",
    "\n",
    "# Run MCMC with the observed data\n",
    "hbm_mcmc.run(\n",
    "    rng_key,\n",
    "    teff_obs=teff_obs,\n",
    "    luminosity_obs=luminosity_obs,\n",
    "    dnufit_obs=dnufit_obs,\n",
    "    FeH_obs=FeH_obs,\n",
    "    G_GAIA_obs=G_GAIA_obs,\n",
    "    massfin_obs=massfin_obs,\n",
    "    numax_obs=numax_obs,\n",
    "    MeH_obs=MeH_obs\n",
    ")\n",
    "\n",
    "# Extract posterior samples\n",
    "posterior_samples = hbm_mcmc.get_samples()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "            FeH[0]     -0.01      0.22     -0.02     -0.35      0.38    701.86      1.00\n",
      "            FeH[1]     -0.01      0.22     -0.02     -0.35      0.35    456.33      1.01\n",
      "            FeH[2]     -0.01      0.22     -0.02     -0.34      0.35    342.10      1.01\n",
      "            FeH[3]     -0.01      0.22     -0.02     -0.40      0.35    634.49      1.00\n",
      "            FeH[4]     -0.00      0.21     -0.02     -0.34      0.37   1045.07      1.00\n",
      "         G_GAIA[0]      0.80      0.05      0.79      0.71      0.88   1803.28      1.00\n",
      "         G_GAIA[1]      0.80      0.05      0.79      0.72      0.89    446.38      1.01\n",
      "         G_GAIA[2]      0.80      0.05      0.79      0.72      0.89   1782.53      1.00\n",
      "         G_GAIA[3]      0.79      0.05      0.79      0.71      0.89    528.98      1.01\n",
      "         G_GAIA[4]      0.80      0.05      0.79      0.72      0.89    721.53      1.00\n",
      "            MeH[0]      0.03      0.11      0.04     -0.14      0.21   1472.14      1.01\n",
      "            MeH[1]      0.03      0.11      0.04     -0.15      0.20   1656.14      1.01\n",
      "            MeH[2]      0.03      0.11      0.04     -0.16      0.20   1694.07      1.01\n",
      "            MeH[3]      0.03      0.11      0.04     -0.15      0.20   1360.70      1.01\n",
      "            MeH[4]      0.03      0.11      0.04     -0.15      0.20   1504.39      1.01\n",
      "         dnufit[0]     12.11      2.61     12.22      7.81     15.92    423.75      1.01\n",
      "         dnufit[1]     10.52      2.77     10.83      5.82     14.63   1646.16      1.00\n",
      "         dnufit[2]     12.53      2.72     12.21      7.85     16.79   1003.27      1.01\n",
      "         dnufit[3]     12.69      2.66     12.82      8.19     16.87   2035.18      1.00\n",
      "         dnufit[4]     10.43      2.77     10.40      5.96     15.13   1401.75      1.00\n",
      "     luminosity[0]     12.23      4.86     12.04      4.32     20.02    395.90      1.01\n",
      "     luminosity[1]     94.58      4.91     94.87     86.71    102.57    901.46      1.00\n",
      "     luminosity[2]     10.03      4.72     10.24      2.21     17.90   2529.73      1.00\n",
      "     luminosity[3]     14.63      5.14     14.64      6.20     22.50    110.71      1.02\n",
      "     luminosity[4]    106.18      4.85    105.98     97.44    113.79   1336.64      1.00\n",
      "        massfin[0]      0.09      0.06      0.10      0.01      0.19     86.36      1.02\n",
      "        massfin[1]      0.10      0.05      0.11      0.01      0.18    231.54      1.02\n",
      "        massfin[2]      0.10      0.05      0.09      0.02      0.19    687.41      1.00\n",
      "        massfin[3]      0.10      0.05      0.09      0.01      0.19    315.45      1.01\n",
      "        massfin[4]      0.09      0.06      0.09     -0.01      0.18     53.19      1.04\n",
      "          mean_FeH     -0.00      0.19     -0.02     -0.37      0.28   1007.77      1.00\n",
      "       mean_G_GAIA      0.80      0.05      0.79      0.72      0.88    949.46      1.00\n",
      "          mean_MeH      0.03      0.10      0.04     -0.14      0.18   1140.96      1.01\n",
      "       mean_dnufit     11.97      2.07     12.09      8.55     15.34   2586.72      1.00\n",
      "   mean_luminosity     57.04     10.74     56.56     39.58     74.82   1631.56      1.00\n",
      "      mean_massfin      0.10      0.05      0.09      0.02      0.18    916.21      1.00\n",
      "        mean_numax      0.02      0.00      0.02      0.02      0.03    132.68      1.01\n",
      "         mean_teff   5092.56    138.92   5073.50   4881.56   5317.75   1095.64      1.00\n",
      "          numax[0]      0.02      0.00      0.02      0.02      0.03    192.79      1.01\n",
      "          numax[1]      0.02      0.00      0.02      0.02      0.03    133.80      1.01\n",
      "          numax[2]      0.02      0.00      0.02      0.02      0.03    323.64      1.01\n",
      "          numax[3]      0.02      0.00      0.02      0.02      0.03    146.71      1.02\n",
      "          numax[4]      0.02      0.00      0.02      0.02      0.03    187.27      1.01\n",
      "         sigma_FeH      0.09      0.06      0.08      0.01      0.17    687.19      1.01\n",
      "      sigma_G_GAIA      0.02      0.01      0.01      0.00      0.03    672.36      1.00\n",
      "         sigma_MeH      0.04      0.03      0.03      0.00      0.08     98.50      1.03\n",
      "      sigma_dnufit      1.93      1.23      1.81      0.05      3.66   1079.60      1.01\n",
      "  sigma_luminosity     29.20      4.66     28.79     21.78     36.37    196.92      1.01\n",
      "     sigma_massfin      0.02      0.01      0.02      0.00      0.04     34.75      1.06\n",
      "       sigma_numax      0.00      0.00      0.00      0.00      0.00    218.14      1.01\n",
      "        sigma_teff    304.08     83.17    290.32    185.71    428.93    130.52      1.02\n",
      "           teff[0]   4931.88      5.17   4931.88   4923.54   4939.81     58.70      1.04\n",
      "           teff[1]   4652.75      5.02   4652.98   4644.59   4660.84    187.58      1.02\n",
      "           teff[2]   4892.00      4.79   4891.95   4884.14   4900.05   1149.60      1.00\n",
      "           teff[3]   5530.74      5.37   5530.83   5522.04   5538.79     59.96      1.03\n",
      "           teff[4]   5091.03      4.60   5090.76   5083.01   5098.35   2575.49      1.00\n",
      "\n",
      "Number of divergences: 766\n"
     ]
    }
   ],
   "source": [
    "hbm_mcmc.print_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
