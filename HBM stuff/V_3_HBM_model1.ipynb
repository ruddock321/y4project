{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro\n",
    "import numpy as np \n",
    "\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS, Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "garstec_data = 'C:\\\\Users\\\\Dell\\\\Downloads\\\\Garstec_AS09_chiara.hdf5'\n",
    "\n",
    "# 7 Inputs\n",
    "ages = []\n",
    "massini = []\n",
    "fehini = []\n",
    "alphamlt = []\n",
    "yini = []\n",
    "eta = []\n",
    "alphafe = []\n",
    "\n",
    "# 8 Outputs\n",
    "teff = []\n",
    "luminosity = []\n",
    "dnufit = []\n",
    "FeH = []\n",
    "G_GAIA = []\n",
    "massfin = []\n",
    "numax = []\n",
    "MeH = []\n",
    "\n",
    "# Open the hdf5 file (read-only mode)\n",
    "with h5py.File(garstec_data, 'r') as hdf:\n",
    "\n",
    "    grid = hdf['grid']\n",
    "    tracks = grid['tracks']\n",
    "\n",
    "    # Get a list of track names and shuffle for random sampling\n",
    "    track_names = list(tracks.keys())\n",
    "    random.seed(1)\n",
    "    random.shuffle(track_names)\n",
    "\n",
    "    # Choose a subset of tracks to process\n",
    "    num_tracks = 1000  # Set the number of tracks to process\n",
    "    selected_tracks = track_names[:num_tracks]\n",
    "\n",
    "    for track_name in selected_tracks:  # Iterate over the selected track names\n",
    "        track = tracks[track_name]\n",
    "        \n",
    "        # Inputs\n",
    "        ages.append(track['age'][:])\n",
    "        massini.append(track['massini'][:])\n",
    "        fehini.append(track['FeHini'][:])\n",
    "        alphamlt.append(track['alphaMLT'][:])\n",
    "        yini.append(track['yini'][:])\n",
    "        eta.append(track['eta'][:])\n",
    "        alphafe.append(track['alphaFe'][:])\n",
    "\n",
    "        # Outputs\n",
    "        teff.append(track['Teff'][:])\n",
    "        luminosity.append(track['LPhot'][:])\n",
    "        dnufit.append(track['dnufit'][:])\n",
    "        FeH.append(track['FeH'][:])\n",
    "        G_GAIA.append(track['G_GAIA'][:])\n",
    "        massfin.append(track['massfin'][:])\n",
    "        numax.append(track['numax'][:])\n",
    "        MeH.append(track['MeH'][:])\n",
    "\n",
    "# Convert lists to numpy arrays and concatenate them (make one big list)\n",
    "\n",
    "# Define a small constant to avoid log10(0)\n",
    "epsilon = 1e-10\n",
    "\n",
    "# Features requiring log10 transformation\n",
    "log10_vars_inputs = [ages, massini, alphamlt, eta, alphafe]\n",
    "\n",
    "# Transform log10 variables\n",
    "log10_transformed_inputs = [np.log10(np.maximum(np.concatenate(var).reshape(-1, 1), epsilon)) for var in log10_vars_inputs]\n",
    "\n",
    "# Concatenate all inputs, including raw `fehini` and `yini`\n",
    "inputs = np.hstack(log10_transformed_inputs + [np.concatenate(fehini).reshape(-1, 1), np.concatenate(yini).reshape(-1, 1)])\n",
    "\n",
    "# Features requiring log10 transformation (strictly positive outputs)\n",
    "log10_vars_outputs = [teff, luminosity, dnufit, G_GAIA, massfin, numax]\n",
    "\n",
    "# Transform log10 variables\n",
    "log10_transformed_outputs = [np.log10(np.maximum(np.concatenate(var).reshape(-1, 1), epsilon)) for var in log10_vars_outputs]\n",
    "\n",
    "# Combine transformed log10 outputs with raw FeH and MeH\n",
    "# FeH and MeH are not transformed, concatenated directly\n",
    "outputs = np.hstack(log10_transformed_outputs + [np.concatenate(FeH).reshape(-1, 1), np.concatenate(MeH).reshape(-1, 1)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.2, random_state=1)\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "y_test = scaler_y.transform(y_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_53256\\3765155684.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('garstec_model_V3_state.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network\n",
    "class GarstecNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GarstecNet, self).__init__()\n",
    "        self.dense1 = nn.Linear(7, 64)   # Input layer\n",
    "        self.dense2 = nn.Linear(64, 64)\n",
    "        self.dense3 = nn.Linear(64, 64)  \n",
    "        self.dense4 = nn.Linear(64, 64)\n",
    "        self.dense5 = nn.Linear(64, 64)\n",
    "        self.dense6 = nn.Linear(64, 8)    # Output layer\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.dense1(x))\n",
    "        x = torch.relu(self.dense2(x))\n",
    "        x = torch.relu(self.dense3(x))  \n",
    "        x = torch.relu(self.dense4(x))\n",
    "        x = torch.relu(self.dense5(x))\n",
    "        x = self.dense6(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# Load the pre-trained model weights\n",
    "model = GarstecNet()\n",
    "model.load_state_dict(torch.load('garstec_model_V3_state.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.9806\n",
      "Mean Absolute Error: 0.0856\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor)\n",
    "    predictions = scaler_y.inverse_transform(predictions.numpy())\n",
    "    y_test_actual = scaler_y.inverse_transform(y_test_tensor.numpy())\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    r2 = r2_score(y_test_actual, predictions)\n",
    "    mae = mean_absolute_error(y_test_actual, predictions)\n",
    "\n",
    "    print(f'R^2 Score: {r2:.4f}')\n",
    "    print(f'Mean Absolute Error: {mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5343.97019187 5290.15096002 4571.04590815 4712.07344939 5202.79638429]\n",
      "[51.83312081 22.73453707 56.96213602 18.31391285 20.48012408]\n",
      "[ 5.44230705 10.7085414   3.5376972   8.11237536 11.60334757]\n",
      "[0.31067038 1.10535541 0.33809111 1.39064558 1.0780014 ]\n",
      "[0.98823883 1.1706095  1.27476752 0.98310528 1.3173898 ]\n",
      "[0.01458654 0.03787579 0.0098701  0.02630836 0.04478054]\n",
      "[0.0137835  0.05111988 1.14555617 0.45702248 0.88437662]\n",
      "[0.01237402 0.07831608 1.82065172 0.49113763 0.73058392]\n"
     ]
    }
   ],
   "source": [
    "# Fixed number of stars\n",
    "n_stars = 5\n",
    "\n",
    "# Dictionary to store predictions\n",
    "predictions = {\n",
    "    'Teff': [],\n",
    "    'Luminosity': [],\n",
    "    'Dnufit': [],\n",
    "    'FeH': [],\n",
    "    'G_GAIA': [],\n",
    "    'Massfin': [],\n",
    "    'Numax': [],\n",
    "    'MeH': []\n",
    "}\n",
    "\n",
    "with h5py.File(garstec_data, 'r') as hdf:\n",
    "    for _ in range(n_stars):\n",
    "        # Select a random track\n",
    "        specific_track_name = np.random.choice(selected_tracks)\n",
    "        specific_track = hdf['grid']['tracks'][specific_track_name]\n",
    "        \n",
    "        # Randomly sample a single data point from the track\n",
    "        # This is because with too many points the model couldnt \"walk\" far enough so simplified it while keeping the\n",
    "        # cluster of stars property of the problem.\n",
    "        num_points = len(specific_track['age'])\n",
    "        idx = np.random.randint(0, num_points)  # Random index\n",
    "        \n",
    "        \n",
    "        # Retrieve features for the sampled point\n",
    "        ages = specific_track['age'][idx].reshape(-1, 1)\n",
    "        massini = specific_track['massini'][idx].reshape(-1, 1)\n",
    "        fehini = specific_track['FeHini'][idx].reshape(-1, 1)\n",
    "        alphamlt = specific_track['alphaMLT'][idx].reshape(-1, 1)\n",
    "        yini = specific_track['yini'][idx].reshape(-1, 1)\n",
    "        eta = specific_track['eta'][idx].reshape(-1, 1)\n",
    "        alphafe = specific_track['alphaFe'][idx].reshape(-1, 1)\n",
    "        \n",
    "        # Combine features into a single array\n",
    "        epsilon = 1e-10\n",
    "        log10_vars_inputs = [ages, massini, alphamlt, eta, alphafe]\n",
    "        log10_transformed_inputs = [np.log10(np.maximum(var, epsilon)) for var in log10_vars_inputs]\n",
    "        all_features = np.hstack(log10_transformed_inputs + [fehini, yini])\n",
    "\n",
    "        # Scale features\n",
    "        all_features_scaled = scaler_X.transform(all_features)\n",
    "        all_features_tensor = torch.FloatTensor(all_features_scaled)\n",
    "        \n",
    "        # Make predictions\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions_specific = model(all_features_tensor).numpy()\n",
    "            predictions_specific = scaler_y.inverse_transform(predictions_specific)\n",
    "\n",
    "\n",
    "        # Extract and store predictions for this star\n",
    "        predictions['Teff'].append(10**predictions_specific[0, 0])\n",
    "        predictions['Luminosity'].append(10**predictions_specific[0, 1])\n",
    "        predictions['Dnufit'].append(10**predictions_specific[0, 2])\n",
    "        predictions['FeH'].append(10**predictions_specific[0, 3])\n",
    "        predictions['G_GAIA'].append(10**predictions_specific[0, 4])\n",
    "        predictions['Massfin'].append(10**predictions_specific[0, 5])\n",
    "        predictions['Numax'].append(10**predictions_specific[0, 6])\n",
    "        predictions['MeH'].append(10**predictions_specific[0, 7])\n",
    "\n",
    "teff_obs = np.array(predictions['Teff'])\n",
    "luminosity_obs = np.array(predictions['Luminosity'])\n",
    "dnufit_obs = np.array(predictions['Dnufit'])\n",
    "FeH_obs = np.array(predictions['FeH'])\n",
    "G_GAIA_obs = np.array(predictions['G_GAIA'])\n",
    "massfin_obs = np.array(predictions['Massfin'])\n",
    "numax_obs = np.array(predictions['Numax'])\n",
    "MeH_obs = np.array(predictions['MeH'])\n",
    "\n",
    "# Checking data points for debugging\n",
    "print(teff_obs)\n",
    "print(luminosity_obs)\n",
    "print(dnufit_obs)\n",
    "print(FeH_obs)\n",
    "print(G_GAIA_obs)\n",
    "print(massfin_obs)\n",
    "print(numax_obs)\n",
    "print(MeH_obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_model(teff_obs, luminosity_obs, dnufit_obs, FeH_obs, \n",
    "                       G_GAIA_obs, massfin_obs, numax_obs, MeH_obs):\n",
    "    # Hyperpriors for group-level parameters\n",
    "    # Picked based on a brief look at the data in order to get the model working, more thought will have to be\n",
    "    # given to these, especially the smaller ones such as FeH etc\n",
    "    mean_teff = numpyro.sample('mean_teff', dist.Normal(6000, 500))\n",
    "    sigma_teff = numpyro.sample('sigma_teff', dist.HalfNormal(200))\n",
    "\n",
    "    mean_luminosity = numpyro.sample('mean_luminosity', dist.Normal(80, 20))\n",
    "    sigma_luminosity = numpyro.sample('sigma_luminosity', dist.HalfNormal(10))\n",
    "\n",
    "    mean_dnufit = numpyro.sample('mean_dnufit', dist.Normal(20, 5))\n",
    "    sigma_dnufit = numpyro.sample('sigma_dnufit', dist.HalfNormal(2))\n",
    "\n",
    "    mean_FeH = numpyro.sample('mean_FeH', dist.Normal(0, 0.2))\n",
    "    sigma_FeH = numpyro.sample('sigma_FeH', dist.HalfNormal(0.1))\n",
    "\n",
    "    mean_G_GAIA = numpyro.sample('mean_G_GAIA', dist.Normal(0.8, 0.05))\n",
    "    sigma_G_GAIA = numpyro.sample('sigma_G_GAIA', dist.HalfNormal(0.02))\n",
    "\n",
    "    mean_massfin = numpyro.sample('mean_massfin', dist.Normal(0.1, 0.05))\n",
    "    sigma_massfin = numpyro.sample('sigma_massfin', dist.HalfNormal(0.02))\n",
    "\n",
    "    mean_numax = numpyro.sample('mean_numax', dist.Normal(0.023, 0.002))\n",
    "    sigma_numax = numpyro.sample('sigma_numax', dist.HalfNormal(0.001))\n",
    "\n",
    "    mean_MeH = numpyro.sample('mean_MeH', dist.Normal(0.025, 0.1))\n",
    "    sigma_MeH = numpyro.sample('sigma_MeH', dist.HalfNormal(0.05))\n",
    "\n",
    "    # Star-level distributions \n",
    "    # Sampling the stars based off of the hyper priors above, using the numpyro plate feature\n",
    "    with numpyro.plate('stars', n_stars):\n",
    "        teff = numpyro.sample('teff', dist.Normal(mean_teff, sigma_teff))\n",
    "        luminosity = numpyro.sample('luminosity', dist.Normal(mean_luminosity, sigma_luminosity))\n",
    "        dnufit = numpyro.sample('dnufit', dist.Normal(mean_dnufit, sigma_dnufit))\n",
    "        FeH = numpyro.sample('FeH', dist.Normal(mean_FeH, sigma_FeH))\n",
    "        G_GAIA = numpyro.sample('G_GAIA', dist.Normal(mean_G_GAIA, sigma_G_GAIA))\n",
    "        massfin = numpyro.sample('massfin', dist.Normal(mean_massfin, sigma_massfin))\n",
    "        numax = numpyro.sample('numax', dist.Normal(mean_numax, sigma_numax))\n",
    "        MeH = numpyro.sample('MeH', dist.Normal(mean_MeH, sigma_MeH))\n",
    "\n",
    "\n",
    "        # Observation noise\n",
    "        obs_sigma = 5.0  # Picked as a starting point but more thought needs to be given\n",
    "        numpyro.sample('obs_teff', dist.Normal(teff, obs_sigma), obs=teff_obs)\n",
    "        numpyro.sample('obs_luminosity', dist.Normal(luminosity, obs_sigma), obs=luminosity_obs)\n",
    "        numpyro.sample('obs_dnufit', dist.Normal(dnufit, obs_sigma), obs=dnufit_obs)\n",
    "        numpyro.sample('obs_FeH', dist.Normal(FeH, obs_sigma), obs=FeH_obs)\n",
    "        numpyro.sample('obs_G_GAIA', dist.Normal(G_GAIA, obs_sigma), obs=G_GAIA_obs)\n",
    "        numpyro.sample('obs_massfin', dist.Normal(massfin, obs_sigma), obs=massfin_obs)\n",
    "        numpyro.sample('obs_numax', dist.Normal(numax, obs_sigma), obs=numax_obs)\n",
    "        numpyro.sample('obs_MeH', dist.Normal(MeH, obs_sigma), obs=MeH_obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_53256\\3580784233.py:6: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n",
      "  hbm_mcmc = MCMC(nuts_kernel, num_samples=4000, num_warmup=4000, num_chains=2, thinning=4)\n",
      "sample: 100%|██████████| 8000/8000 [00:46<00:00, 171.04it/s, 3 steps of size 5.97e-02. acc. prob=0.45]   \n",
      "sample: 100%|██████████| 8000/8000 [01:37<00:00, 82.02it/s, 255 steps of size 1.17e-02. acc. prob=0.92]  \n"
     ]
    }
   ],
   "source": [
    "from jax import random\n",
    "from numpyro.infer import NUTS, MCMC\n",
    "\n",
    "# Set up the NUTS kernel and MCMC\n",
    "nuts_kernel = NUTS(hierarchical_model)\n",
    "hbm_mcmc = MCMC(nuts_kernel, num_samples=4000, num_warmup=4000, num_chains=2, thinning=4)\n",
    "\n",
    "# Random seed\n",
    "rng_key = random.PRNGKey(0)\n",
    "\n",
    "# Run MCMC with the observed data\n",
    "hbm_mcmc.run(\n",
    "    rng_key,\n",
    "    teff_obs=teff_obs,\n",
    "    luminosity_obs=luminosity_obs,\n",
    "    dnufit_obs=dnufit_obs,\n",
    "    FeH_obs=FeH_obs,\n",
    "    G_GAIA_obs=G_GAIA_obs,\n",
    "    massfin_obs=massfin_obs,\n",
    "    numax_obs=numax_obs,\n",
    "    MeH_obs=MeH_obs\n",
    ")\n",
    "\n",
    "# Extract posterior samples\n",
    "posterior_samples = hbm_mcmc.get_samples()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "            FeH[0]      0.01      0.21      0.01     -0.35      0.37    729.43      1.00\n",
      "            FeH[1]      0.03      0.22      0.01     -0.32      0.37    221.18      1.02\n",
      "            FeH[2]      0.02      0.21      0.02     -0.34      0.36    812.37      1.01\n",
      "            FeH[3]      0.03      0.22      0.01     -0.35      0.35    148.37      1.02\n",
      "            FeH[4]      0.02      0.22      0.01     -0.33      0.37    479.99      1.01\n",
      "         G_GAIA[0]      0.80      0.05      0.80      0.72      0.89    276.01      1.01\n",
      "         G_GAIA[1]      0.80      0.05      0.81      0.72      0.89    218.04      1.01\n",
      "         G_GAIA[2]      0.80      0.05      0.80      0.71      0.88    244.54      1.01\n",
      "         G_GAIA[3]      0.80      0.05      0.80      0.72      0.89    568.02      1.00\n",
      "         G_GAIA[4]      0.80      0.05      0.80      0.72      0.89    214.54      1.02\n",
      "            MeH[0]      0.01      0.11      0.00     -0.14      0.20    255.40      1.00\n",
      "            MeH[1]      0.01      0.11     -0.00     -0.14      0.20    197.95      1.00\n",
      "            MeH[2]      0.02      0.11      0.01     -0.17      0.18    276.66      1.00\n",
      "            MeH[3]      0.01      0.11      0.00     -0.14      0.20    278.60      1.00\n",
      "            MeH[4]      0.01      0.11      0.00     -0.16      0.19    255.08      1.00\n",
      "         dnufit[0]      9.27      2.55      9.40      5.01     13.10     51.69      1.05\n",
      "         dnufit[1]      9.72      2.56      9.54      5.36     13.60     65.60      1.04\n",
      "         dnufit[2]      9.20      2.47      9.35      5.16     12.89    105.18      1.03\n",
      "         dnufit[3]      9.51      2.48      9.48      5.24     13.23     65.62      1.04\n",
      "         dnufit[4]      9.97      2.63      9.96      5.67     14.05     69.24      1.05\n",
      "     luminosity[0]     49.86      4.70     49.60     42.53     57.04    130.20      1.02\n",
      "     luminosity[1]     24.27      4.90     24.54     16.26     31.68     86.12      1.04\n",
      "     luminosity[2]     55.02      4.89     54.95     47.64     62.81    121.77      1.03\n",
      "     luminosity[3]     20.30      4.97     20.21     12.27     27.60    149.93      1.01\n",
      "     luminosity[4]     22.21      4.56     22.18     15.18     29.83    807.58      1.01\n",
      "        massfin[0]      0.10      0.05      0.10      0.02      0.17    275.90      1.01\n",
      "        massfin[1]      0.10      0.05      0.10      0.01      0.17    313.51      1.01\n",
      "        massfin[2]      0.10      0.05      0.10      0.02      0.18    274.95      1.01\n",
      "        massfin[3]      0.10      0.05      0.10      0.03      0.19    288.66      1.01\n",
      "        massfin[4]      0.10      0.05      0.10      0.02      0.18    263.80      1.01\n",
      "          mean_FeH      0.02      0.19      0.01     -0.30      0.32    294.15      1.01\n",
      "       mean_G_GAIA      0.80      0.05      0.80      0.72      0.88    309.32      1.00\n",
      "          mean_MeH      0.01      0.10      0.01     -0.12      0.19    189.47      1.00\n",
      "       mean_dnufit      9.75      2.26      9.80      5.47     12.90     55.03      1.05\n",
      "   mean_luminosity     39.69      7.57     39.32     28.18     52.30    551.94      1.00\n",
      "      mean_massfin      0.10      0.05      0.10      0.03      0.17    226.07      1.01\n",
      "        mean_numax      0.02      0.00      0.02      0.02      0.03    641.59      1.01\n",
      "         mean_teff   5100.55    169.76   5105.24   4785.16   5320.29     63.23      1.06\n",
      "          numax[0]      0.02      0.00      0.02      0.02      0.03    827.62      1.00\n",
      "          numax[1]      0.02      0.00      0.02      0.02      0.03    465.73      1.01\n",
      "          numax[2]      0.02      0.00      0.02      0.02      0.03    589.43      1.01\n",
      "          numax[3]      0.02      0.00      0.02      0.02      0.03    162.80      1.03\n",
      "          numax[4]      0.02      0.00      0.02      0.02      0.03    902.03      1.00\n",
      "         sigma_FeH      0.08      0.06      0.07      0.01      0.17    177.02      1.01\n",
      "      sigma_G_GAIA      0.02      0.01      0.02      0.00      0.03    105.63      1.02\n",
      "         sigma_MeH      0.04      0.03      0.03      0.00      0.08    210.05      1.01\n",
      "      sigma_dnufit      1.36      1.05      1.06      0.03      2.86    127.33      1.02\n",
      "  sigma_luminosity     15.61      4.55     14.97      8.71     22.20    115.42      1.02\n",
      "     sigma_massfin      0.01      0.01      0.01      0.00      0.03    110.32      1.01\n",
      "       sigma_numax      0.00      0.00      0.00      0.00      0.00    654.46      1.00\n",
      "        sigma_teff    343.58     88.13    336.23    208.36    475.28     23.27      1.03\n",
      "           teff[0]   5343.90      4.85   5344.05   5335.92   5351.69    486.79      1.01\n",
      "           teff[1]   5290.64      5.37   5290.21   5281.92   5300.48     73.75      1.04\n",
      "           teff[2]   4571.29      4.67   4571.58   4562.98   4578.51    579.42      1.00\n",
      "           teff[3]   4712.58      4.72   4712.43   4705.42   4720.27    657.78      1.00\n",
      "           teff[4]   5202.97      4.59   5203.38   5194.45   5209.53    403.33      1.01\n",
      "\n",
      "Number of divergences: 495\n"
     ]
    }
   ],
   "source": [
    "hbm_mcmc.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.infer import Predictive\n",
    "\n",
    "# Create a Predictive object for posterior predictive sampling\n",
    "predictive = Predictive(\n",
    "    hierarchical_model, \n",
    "    posterior_samples=hbm_mcmc.get_samples()\n",
    ")\n",
    "\n",
    "# Generate posterior predictive samples\n",
    "ppc_samples = predictive(\n",
    "    random.PRNGKey(1), \n",
    "    teff_obs=teff_obs, \n",
    "    luminosity_obs=luminosity_obs, \n",
    "    dnufit_obs=dnufit_obs, \n",
    "    FeH_obs=FeH_obs, \n",
    "    G_GAIA_obs=G_GAIA_obs, \n",
    "    massfin_obs=massfin_obs, \n",
    "    numax_obs=numax_obs, \n",
    "    MeH_obs=MeH_obs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import numpy as np\n",
    "\n",
    "inference_data = az.from_numpyro(posterior=hbm_mcmc, posterior_predictive=ppc_samples)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Plot posterior predictive checks for teff\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m az\u001b[38;5;241m.\u001b[39mplot_ppc(inference_data, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkde\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\arviz\\plots\\ppcplot.py:368\u001b[0m, in \u001b[0;36mplot_ppc\u001b[1;34m(data, kind, alpha, mean, observed, observed_rug, color, colors, grid, figsize, textsize, data_pairs, var_names, filter_vars, coords, flatten, flatten_pp, num_pp_samples, random_seed, jitter, animated, animation_kwargs, legend, labeller, ax, backend, backend_kwargs, group, show)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# TODO: Add backend kwargs\u001b[39;00m\n\u001b[0;32m    367\u001b[0m plot \u001b[38;5;241m=\u001b[39m get_plotting_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplot_ppc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppcplot\u001b[39m\u001b[38;5;124m\"\u001b[39m, backend)\n\u001b[1;32m--> 368\u001b[0m axes \u001b[38;5;241m=\u001b[39m plot(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mppcplot_kwargs)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m axes\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\arviz\\plots\\backends\\matplotlib\\ppcplot.py:166\u001b[0m, in \u001b[0;36mplot_ppc\u001b[1;34m(ax, length_plotters, rows, cols, figsize, animated, obs_plotters, pp_plotters, predictive_dataset, pp_sample_ix, kind, alpha, colors, textsize, mean, observed, observed_rug, jitter, total_pp_samples, legend, labeller, group, animation_kwargs, num_pp_samples, backend_kwargs, show)\u001b[0m\n\u001b[0;32m    164\u001b[0m vals \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([vals])\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 166\u001b[0m     pp_x, pp_density \u001b[38;5;241m=\u001b[39m kde(vals)\n\u001b[0;32m    167\u001b[0m     pp_densities\u001b[38;5;241m.\u001b[39mappend(pp_density)\n\u001b[0;32m    168\u001b[0m     pp_xs\u001b[38;5;241m.\u001b[39mappend(pp_x)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\arviz\\stats\\density_utils.py:499\u001b[0m, in \u001b[0;36mkde\u001b[1;34m(x, circular, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    497\u001b[0m     kde_fun \u001b[38;5;241m=\u001b[39m _kde_linear\n\u001b[1;32m--> 499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kde_fun(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\arviz\\stats\\density_utils.py:585\u001b[0m, in \u001b[0;36m_kde_linear\u001b[1;34m(x, bw, adaptive, extend, bound_correction, extend_fct, bw_fct, bw_return, custom_lims, cumulative, grid_len, **kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m grid_counts, _, grid_edges \u001b[38;5;241m=\u001b[39m histogram(x, grid_len, (grid_min, grid_max))\n\u001b[0;32m    584\u001b[0m \u001b[38;5;66;03m# Bandwidth estimation\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m bw \u001b[38;5;241m=\u001b[39m bw_fct \u001b[38;5;241m*\u001b[39m _get_bw(x, bw, grid_counts, x_std, x_range)\n\u001b[0;32m    587\u001b[0m \u001b[38;5;66;03m# Density estimation\u001b[39;00m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m adaptive:\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\arviz\\stats\\density_utils.py:158\u001b[0m, in \u001b[0;36m_get_bw\u001b[1;34m(x, bw, grid_counts, x_std, x_range)\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    152\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized bandwidth method.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbw_lower\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    154\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected one of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(_BW_METHODS_LINEAR)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m         )\n\u001b[0;32m    157\u001b[0m     bw_fun \u001b[38;5;241m=\u001b[39m _BW_METHODS_LINEAR[bw_lower]\n\u001b[1;32m--> 158\u001b[0m     bw \u001b[38;5;241m=\u001b[39m bw_fun(x, grid_counts\u001b[38;5;241m=\u001b[39mgrid_counts, x_std\u001b[38;5;241m=\u001b[39mx_std, x_range\u001b[38;5;241m=\u001b[39mx_range)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized `bw` argument.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a positive numeric or one of the following strings:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(_BW_METHODS_LINEAR)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\arviz\\stats\\density_utils.py:82\u001b[0m, in \u001b[0;36m_bw_experimental\u001b[1;34m(x, grid_counts, x_std, x_range)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Experimental bandwidth estimator.\"\"\"\u001b[39;00m\n\u001b[0;32m     81\u001b[0m bw_silverman \u001b[38;5;241m=\u001b[39m _bw_silverman(x, x_std\u001b[38;5;241m=\u001b[39mx_std)\n\u001b[1;32m---> 82\u001b[0m bw_isj \u001b[38;5;241m=\u001b[39m _bw_isj(x, grid_counts\u001b[38;5;241m=\u001b[39mgrid_counts, x_range\u001b[38;5;241m=\u001b[39mx_range)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (bw_silverman \u001b[38;5;241m+\u001b[39m bw_isj)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\arviz\\stats\\density_utils.py:69\u001b[0m, in \u001b[0;36m_bw_isj\u001b[1;34m(x, grid_counts, x_std, x_range)\u001b[0m\n\u001b[0;32m     66\u001b[0m grid_relfreq \u001b[38;5;241m=\u001b[39m grid_counts \u001b[38;5;241m/\u001b[39m x_len\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Discrete cosine transform of the data\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m a_k \u001b[38;5;241m=\u001b[39m _dct1d(grid_relfreq)\n\u001b[0;32m     71\u001b[0m k_sq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, grid_len) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     72\u001b[0m a_sq \u001b[38;5;241m=\u001b[39m a_k[\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, grid_len)] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\arviz\\stats\\density_utils.py:218\u001b[0m, in \u001b[0;36m_dct1d\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    214\u001b[0m odd_decreasing \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(x_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    216\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((x[even_increasing], x[odd_decreasing]))\n\u001b[1;32m--> 218\u001b[0m w_1k \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mr_[\u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m(\u001b[38;5;241m0\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39mj) \u001b[38;5;241m*\u001b[39m (np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, x_len)) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m x_len)))]\n\u001b[0;32m    219\u001b[0m output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreal(w_1k \u001b[38;5;241m*\u001b[39m fft(x))\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot posterior predictive checks for teff\n",
    "az.plot_ppc(inference_data, kind=\"kde\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
